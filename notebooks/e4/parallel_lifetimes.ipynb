{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4133156e",
   "metadata": {},
   "source": [
    "# SlurmCluster on E4\n",
    "\n",
    "Available systems:\n",
    "\n",
    "- Arm Frontend: 172.18.16.70 (tlnode01.e4red)\n",
    "- Intel Frontend*: 172.18.19.216 (ilnode01.e4red)\n",
    "- AMD Frontend*:* 172.18.16.79 (alnode01.e4red)\n",
    "\n",
    "## Measuring power consumption\n",
    "\n",
    "On E4, power consumption can be measured, but only `icnode01` is connected to a power meter. When queueing, this host has to be selected by passing the `--nodelist icnode01` flag to Slurm. The power measurements can be written to a CSV file via the `/opt/share/sw/zes/power.py`script, which has to be run with `sudo`. Using the `-f` flag, the file path can be specified. It has to be started before submitting the job and writes the power consumption on the fly.\n",
    "\n",
    "```commandline\n",
    "sudo /opt/share/sw/zes/power.py -f <path to outfile>.csv\n",
    "\n",
    "```\n",
    "\n",
    "## Account-partition options\n",
    "\n",
    "The command `sacctmgr show user <user name> withassoc` shows all available account-partition combinations that can be used with Slurm. \n",
    "  - `*-hw` partitions are designed for short jobs (<=1h) and have 2 nodes\n",
    "  - `*-lw` partitions are designed for long jobs (1h-3d) and have 4 nodes\n",
    "  - `ice-nc` partition is designed for power consumption measurement and has 2 nodes.\n",
    "  - `*-builder` partitions are designed for very small jobs that only require very few resources such as compiling a program or compressing an archive. Each job can request no more than 6 cores and 16GB of memory.\n",
    "\n",
    "Partitions:\n",
    "  - Intel architecture: `casc-hw`, `casc-lw`, and `ice-nc` (for power consumption measurement)\n",
    "  - AMD architecture: `mil-hw` and `mil-lw`\n",
    "\n",
    "Accounts:\n",
    "  - The name of the project account is `maelstrom`.\n",
    "  - The name of the account that can be used with the `*-builder` partitions is `builder`.\n",
    "  \n",
    " \n",
    "## Configuring and running a Slurm Cluster\n",
    "\n",
    " - Here we use a client for clusters on E4 that uses dask in the backend.\n",
    " - The client runs commands inside Singularity containers. The path to the Singularity image has to be given as an environemnt variable `lifetimes.parallel.slurm.SINGULARITY_IMAGE_ENV_VAR`, e.g. as\n",
    "   ```Python\n",
    "   import os\n",
    "\n",
    "   os.environ[lifetimes.parallel.slurm.SINGULARITY_IMAGE_ENV_VAR] = \"<path to Singularity image file>\"\n",
    "\n",
    "   ```\n",
    "   The executable of the dask workers can also be overwritten, though. \n",
    "   E.g. a virtual environment could be loaded and the Python executable be used:\n",
    "   ```Python\n",
    "   client = lifetimes.parallel.slurm.E4Client(\n",
    "       ...\n",
    "       extra_job_commands=[\". /<path to venv>/bin/activate\"],\n",
    "       python_executable=[\"python3\"],\n",
    "   )\n",
    "   ```\n",
    " - Dask will convert all configurations to a batch script which can be inspected via `E4Client.job_script`.\n",
    " - The `port` argument of `E4Client` defines the port used for the dask dashboard. The dashboard can be accessed via `https://<host ip>:<port>`.\n",
    " \n",
    "## Running a distributed program\n",
    "\n",
    " - The `E4Client` instance is initialized with 2 workers by default, running on one node. `with client.scale(...):` enables the user to scale the cluster to the desired capacity. In the notebook, just typing `client.cluster` and hitting return will provide you with a widget in which you can scale your cluster as well.\n",
    " - Dask supports lazy evaluation; more concretely, graph building and graph execution are separated. In your notebook, you can define any function, that computes on a `dask.array`. The function can be called with arguments to give you the graph. `func(*args, **kwargs).compute()` will trigger actual execution. If a cluster is configured and requested in a context via `with client.scale(...)`, the execution is distributed by dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b072b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import lifetimes\n",
    "\n",
    "lifetimes.utils.log_to_stdout()\n",
    "\n",
    "os.environ[lifetimes.parallel.slurm.SINGULARITY_IMAGE_ENV_VAR] = \"/home/femmerich/.singularity/jupyter-kernel.sif\"\n",
    "\n",
    "data_path = \"/data/maelstrom/a6/temperature_level_128_daily_averages_2017_2020.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80ea02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = lifetimes.parallel.slurm.E4Client(\n",
    "    queue=\"ice-nc\",\n",
    "    project=\"maelstrom\",\n",
    "    cores=16,\n",
    "    memory=64,\n",
    "    processes=4,\n",
    "    walltime=\"02:00:00\",\n",
    "    extra_slurm_options=[\"--nodelist icnode01\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24392eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_ratio = [None]\n",
    "n_clusters = [29]\n",
    "use_varimax = [True]\n",
    "\n",
    "method = functools.partial(\n",
    "    lifetimes.benchmark.wrap_benchmark_method_with_logging(lifetimes.pca_and_kmeans),\n",
    "    data_path,\n",
    ")\n",
    "\n",
    "arguments = itertools.product(variance_ratio, n_clusters, use_varimax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059a92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with client.scale(workers=1):\n",
    "    results = client.execute(method, arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b365fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce974ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
