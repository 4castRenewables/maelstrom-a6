{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3224fe85-9303-405a-842f-d37f609ced79",
   "metadata": {},
   "source": [
    "# PCA + k-Means on ERA5 Data\n",
    "\n",
    "## Results\n",
    "\n",
    "- Data loading takes ~5min\n",
    "- Data preprocessing takes ~4min\n",
    "- PCA with 500 PCs takes ~8min\n",
    "- PCA (reduced PCs), Transforming and clustering (2 x k-Means) takes ~2min\n",
    "- kPCA with 500 PCs takes ~2min\n",
    "- kPCA (reduced PCs), Transforming and clustering (2 x k-Means) takes ~3min\n",
    "- PCA:\n",
    "  - N_pcs=24 cover 0.7 of the variance\n",
    "  - N_pcs=80 cover 0.8 of the variance\n",
    "  - N_pcs=171 cover 0.85 of the variance\n",
    "  - N_pcs=421 cover 0.9 of the variance\n",
    "- kPCA:\n",
    "  - N_pcs=1 cover 0.4 of the variance\n",
    "  - N_pcs=2 cover 0.5 of the variance\n",
    "  - N_pcs=3 cover 0.6000000000000001 of the variance\n",
    "  - N_pcs=6 cover 0.7000000000000001 of the variance\n",
    "  - N_pcs=15 cover 0.8 of the variance\n",
    "  - N_pcs=54 cover 0.9 of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c399384-faab-40e3-8b72-ce854b99d5eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import datetime\n",
    "import pathlib\n",
    "import joblib\n",
    "\n",
    "import sklearn.cluster\n",
    "import sklearn.decomposition\n",
    "import sklearn.preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import a6\n",
    "import a6.plotting._colors as _colors\n",
    "\n",
    "path = pathlib.Path(\n",
    "    \"/p/project/deepacf/emmerich1/data/ecmwf_era5/era5_pl_1964_2023_12.nc\"\n",
    ")\n",
    "plots = pathlib.Path(\"/p/project/deepacf/emmerich1/plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8901c-6af3-44d2-a689-7bfe58b49de4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(path)\n",
    "\n",
    "coordinates = a6.datasets.coordinates.Coordinates()\n",
    "variables = a6.datasets.variables.Model()\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b82c29b-257d-46ae-bd34-342981d7c0d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "masked = a6.datasets.methods.mask.set_nans_to_mean(ds, coordinates=coordinates)\n",
    "data = (\n",
    "    (\n",
    "        a6.features.methods.weighting.weight_by_latitudes(\n",
    "            latitudes=coordinates.latitude,\n",
    "            use_sqrt=True,\n",
    "        )\n",
    "        >> a6.features.methods.reshape.xarray.reshape_spatio_temporal_data(\n",
    "            # Set to None to avoid memory excess in function\n",
    "            time_coordinate=None,\n",
    "        )\n",
    "        >> a6.features.methods.standardization.normalize_features()\n",
    "    )\n",
    "    .apply_to(masked)\n",
    "    .compute()\n",
    ")\n",
    "del ds\n",
    "del masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb40d205-2611-414e-8269-53b6ad319402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# data.to_netcdf(\"/p/project/deepacf/emmerich1/data/ecmwf_era5/era5_pl_1964_2023_12_preprocssed_for_pca.nc\")\n",
    "# del data\n",
    "\n",
    "data = (\n",
    "    xr.open_dataset(\n",
    "        \"/p/project/deepacf/emmerich1/data/ecmwf_era5/era5_pl_1964_2023_12_preprocssed_for_pca.nc\"\n",
    "    )\n",
    "    .to_dataarray()\n",
    "    .values[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b673b37-0dfb-48d8-a689-deeeb61cd732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pca = sklearn.decomposition.PCA(n_components=80).fit(data)\n",
    "joblib.dump(pca, \"/p/project/deepacf/emmerich1/data/pca_500_pcs.joblib\")\n",
    "\n",
    "# pca = joblib.load(\"/p/project/deepacf/emmerich1/data/pca_500_pcs.joblib\")\n",
    "cum_evr = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e7f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import sklearn.utils.sparsefuncs\n",
    "\n",
    "\n",
    "def transform(pca, X, n_components: int):\n",
    "    X = pca._validate_data(\n",
    "        X,\n",
    "        accept_sparse=(\"csr\", \"csc\"),\n",
    "        dtype=[np.float64, np.float32],\n",
    "        reset=False,\n",
    "    )\n",
    "    if pca.mean_ is not None:\n",
    "        if scipy.sparse.issparse(X):\n",
    "            X = sklearn.utils.sparsefuncs._implicit_column_offset(X, pca.mean_)\n",
    "        else:\n",
    "            X = X - pca.mean_\n",
    "    X_transformed = X @ pca.components_[:n_components, :].T\n",
    "    if pca.whiten:\n",
    "        X_transformed /= xp.sqrt(pca.explained_variance_[:n_components])\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eafdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_pcs in range(1, 3):\n",
    "    print(f\"n_pcs={n_pcs}\", end=\"\\r\")\n",
    "    transformed = transform(pca=pca, X=data, n_components=n_pcs)\n",
    "    transformed = sklearn.preprocessing.StandardScaler().fit_transform(\n",
    "        transformed\n",
    "    )\n",
    "    for k in range(1, 3):\n",
    "        print(f\"k={k}\", end=\"\\r\")\n",
    "        kmeans = sklearn.cluster.KMeans(n_clusters=k).fit(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b748c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for n_pcs in range(1, 3):\n",
    "    print(\"n_pcs=\")\n",
    "    print(f\"n_pcs={n_pcs}\", end=\"\\r\")\n",
    "    # transformed = sklearn.decomposition.PCA(n_components=n_pcs).fit_transform(data)\n",
    "    transformed = sklearn.decomposition.KernelPCA(\n",
    "        n_components=n_pcs,\n",
    "        copy_X=False,\n",
    "    ).fit_transform(data)\n",
    "    transformed = sklearn.preprocessing.StandardScaler().fit_transform(\n",
    "        transformed\n",
    "    )\n",
    "    for k in range(1, 3):\n",
    "        print(\"k=\")\n",
    "        print(f\"k={k}\", end=\"\\r\")\n",
    "        kmeans = sklearn.cluster.KMeans(n_clusters=k).fit(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ba943-ab8b-45c0-ba9c-e6db20dba516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "plt.plot(list(range(pca.n_components_)), cum_evr)\n",
    "for var in np.arange(0, 1.0, 0.1):\n",
    "    n_pcs = np.where(cum_evr > var)[0][0]\n",
    "    print(f\"N_pcs={n_pcs} cover {var} of the variance\")\n",
    "n_pcs = np.where(cum_evr > 0.80)[0][0]\n",
    "transformed = sklearn.decomposition.PCA(n_components=n_pcs).fit_transform(data)\n",
    "transformed_standardized = sklearn.preprocessing.StandardScaler().fit_transform(\n",
    "    transformed\n",
    ")\n",
    "kmeans_pca_40 = sklearn.cluster.KMeans(n_clusters=40).fit(\n",
    "    transformed_standardized\n",
    ")\n",
    "kmeans_pca_30 = sklearn.cluster.KMeans(n_clusters=30).fit(\n",
    "    transformed_standardized\n",
    ")\n",
    "\n",
    "kmeans_pca_40_labels = kmeans_pca_40.labels_\n",
    "kmeans_pca_30_labels = kmeans_pca_30.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4222fc-1858-4335-acd6-9700590994b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "joblib.dump(\n",
    "    kmeans_pca_40, \"/p/project/deepacf/emmerich1/data/kmeans_pca_40.joblib\"\n",
    ")\n",
    "joblib.dump(\n",
    "    kmeans_pca_30, \"/p/project/deepacf/emmerich1/data/kmeans_pca_30.joblib\"\n",
    ")\n",
    "\n",
    "del pca\n",
    "del transformed\n",
    "del kmeans_pca_40\n",
    "del kmeans_pca_30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c04eea-f999-440d-8ab6-dbdc94b8e414",
   "metadata": {},
   "source": [
    "Kernel PCA: Gaussian radial basis function with $\\sigma = 200$ (see http://dx.doi.org/10.1016/j.procs.2011.08.043)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342af78d-e030-4dd5-a739-87da35235122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kpca = sklearn.decomposition.KernelPCA(\n",
    "    n_components=500,\n",
    "    copy_X=False,\n",
    ").fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89ee4d-822e-45ca-8074-c4fef0663c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cum_evr = np.cumsum(kpca.eigenvalues_ / np.sum(kpca.eigenvalues_))\n",
    "plt.plot(list(range(len(kpca.eigenvalues_))), cum_evr)\n",
    "for var in np.arange(0, 1.0, 0.1):\n",
    "    n_pcs = np.where(cum_evr > var)[0][0]\n",
    "    print(f\"N_pcs={n_pcs} cover {var} of the variance\")\n",
    "n_pcs = np.where(cum_evr > 0.80)[0][0]\n",
    "transformed = sklearn.decomposition.KernelPCA(\n",
    "    n_components=n_pcs, copy_X=False\n",
    ").fit_transform(data)\n",
    "kmeans_kpca_40 = sklearn.cluster.KMeans(n_clusters=40).fit(transformed)\n",
    "kmeans_kpca_30 = sklearn.cluster.KMeans(n_clusters=30).fit(transformed)\n",
    "\n",
    "kmeans_kpca_40_labels = kmeans_pca_40.labels_\n",
    "kmeans_kpca_30_labels = kmeans_pca_30.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5662d12-0c9d-4850-a77c-edde44cad229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "joblib.dump(kpca, \"/p/project/deepacf/emmerich1/data/kpca_500_pcs.joblib\")\n",
    "joblib.dump(\n",
    "    kmeans_kpca_40, \"/p/project/deepacf/emmerich1/data/kmeans_kpca_40.joblib\"\n",
    ")\n",
    "joblib.dump(\n",
    "    kmeans_kpca_30, \"/p/project/deepacf/emmerich1/data/kmeans_kpca_30.joblib\"\n",
    ")\n",
    "\n",
    "del kpca\n",
    "del transformed\n",
    "del kmeans_kpca_40\n",
    "del kmeans_kpca_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa70ea-c233-40d4-80ef-4df17fb8ffb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = xr.open_dataset(path)\n",
    "\n",
    "# Add +1 to be conform to the GWL labels\n",
    "kmeans_pca_40 = joblib.load(\n",
    "    \"/p/project/deepacf/emmerich1/data/kmeans_pca_40.joblib\"\n",
    ")\n",
    "kmeans_pca_30 = joblib.load(\n",
    "    \"/p/project/deepacf/emmerich1/data/kmeans_pca_30.joblib\"\n",
    ")\n",
    "kmeans_pca_40_labels = kmeans_pca_40.labels_ + 1\n",
    "kmeans_pca_30_labels = kmeans_pca_30.labels_ + 1\n",
    "\n",
    "kmeans_kpca_40 = joblib.load(\n",
    "    \"/p/project/deepacf/emmerich1/data/kmeans_kpca_40.joblib\"\n",
    ")\n",
    "kmeans_kpca_30 = joblib.load(\n",
    "    \"/p/project/deepacf/emmerich1/data/kmeans_kpca_30.joblib\"\n",
    ")\n",
    "kmeans_kpca_40_labels = kmeans_kpca_40.labels_ + 1\n",
    "kmeans_kpca_30_labels = kmeans_kpca_30.labels_ + 1\n",
    "\n",
    "# Need to convert dates to YYYY-MM-DD to be conform with GWL dataset time stamps.\n",
    "times = a6.utils.times.time_steps_as_dates(ds, coordinates=coordinates)\n",
    "\n",
    "results = xr.Dataset(\n",
    "    data_vars={\n",
    "        \"PCA\": (\n",
    "            [\"time\", \"k\"],\n",
    "            list(zip(kmeans_pca_30_labels, kmeans_pca_40_labels)),\n",
    "        ),\n",
    "        \"kPCA\": (\n",
    "            [\"time\", \"k\"],\n",
    "            list(zip(kmeans_kpca_30_labels, kmeans_kpca_40_labels)),\n",
    "        ),\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": times,\n",
    "        \"k\": [30, 40],\n",
    "    },\n",
    "    attrs={\n",
    "        \"description\": \"Resulting LSWR labels from K-Means clustering on PCA and kernel PCA of the ERA5 data (1964-2023)\",\n",
    "    },\n",
    ")\n",
    "results.to_netcdf(\n",
    "    \"/p/project/deepacf/emmerich1/data/pca_kpca_kmeans_lswrs_30_40.nc\"\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e0ee0f-cfa0-4618-bf48-1d2cf490f0cf",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf842b5-203c-4ea8-a9bd-d35a0e22a120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = xr.open_dataset(\n",
    "    \"/p/project/deepacf/emmerich1/data/pca_kpca_kmeans_lswrs_30_40.nc\"\n",
    ")\n",
    "\n",
    "n_lswr_categories = 40\n",
    "results_40 = results.sel(k=n_lswr_categories)\n",
    "gwl = xr.open_dataset(\n",
    "    \"/p/home/jusers/emmerich1/juwels/code/a6/src/tests/data/gwl.nc\"\n",
    ")\n",
    "\n",
    "# NOTE: Uncomment to save dcv2 to disk, or read from disk\n",
    "# dcv2 = torch.load(\n",
    "#     \"/p/scratch/deepacf/emmerich1/dcv2/multi-level-all-fields-1964-2023/results/tensors/epoch-799-assignments.pt\",\n",
    "#     map_location=torch.device(\"cpu\"),\n",
    "# )\n",
    "# dcv2 = xr.DataArray(\n",
    "#     # Need to add +1 to be conform with GWL labels\n",
    "#     dcv2.numpy()[0] + 1,\n",
    "#     name=\"DCv2\",\n",
    "#     coords={\"time\": results[\"time\"]},\n",
    "#     dims=[\"time\"],\n",
    "# )\n",
    "# dcv2.to_netcdf(\"/p/project/deepacf/emmerich1/data/dcv2-lswrs.nc\")\n",
    "\n",
    "dcv2 = xr.open_dataset(\"/p/project/deepacf/emmerich1/data/dcv2-lswrs.nc\")[\n",
    "    \"DCv2\"\n",
    "]\n",
    "\n",
    "lswrs = [gwl[\"GWL\"], results_40[\"PCA\"], results_40[\"kPCA\"], dcv2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae4e6c-a69e-4d80-a7c3-2a2a4af38f41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = joblib.load(\"/p/project/deepacf/emmerich1/data/pca_500_pcs.joblib\")\n",
    "kpca = joblib.load(\"/p/project/deepacf/emmerich1/data/kpca_500_pcs.joblib\")\n",
    "\n",
    "pca_cum_evr = np.cumsum(pca.explained_variance_ratio_)\n",
    "kpca_evr = kpca.eigenvalues_ / np.sum(kpca.eigenvalues_)\n",
    "kpca_cum_evr = np.cumsum(kpca_evr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed3dd56-fc36-48a4-a6a7-4cf1c1fcb8b5",
   "metadata": {},
   "source": [
    "## Explained Variance Ratio Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff9bf5e-5321-4343-89b0-9fede28110ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax1.set_xlabel(\"PCs\")\n",
    "\n",
    "x = list(range(1, pca.n_components + 1))\n",
    "\n",
    "# Plot cumulative variance on first axis\n",
    "ax1_color = \"blue\"\n",
    "ax1.set_ylabel(\"cumulative explained variance\", color=ax1_color)\n",
    "ax1.plot(x, pca_cum_evr, color=ax1_color, linestyle=\"-\", label=\"PCA\")\n",
    "ax1.plot(x, kpca_cum_evr, color=ax1_color, linestyle=\"-.\", label=\"kPCA\")\n",
    "\n",
    "# Create right axis.\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the explained variance ratios.\n",
    "ax2_color = \"green\"\n",
    "ax2.set_ylabel(\"explained variance ratio\", color=ax2_color)\n",
    "ax2.plot(\n",
    "    x,\n",
    "    pca.explained_variance_ratio_,\n",
    "    color=ax2_color,\n",
    "    linestyle=\"-\",\n",
    "    label=\"PCA\",\n",
    ")\n",
    "ax2.plot(x, kpca_evr, color=ax2_color, linestyle=\"-.\", label=\"kPCA\")\n",
    "\n",
    "for ax, color in [(ax1, ax1_color), (ax2, ax2_color)]:\n",
    "    # Set log scale.\n",
    "    ax.set(xscale=\"log\", yscale=\"log\")\n",
    "    # Set left xlim such that the first tick disappears.\n",
    "    ax.set_xlim(0.91, None)\n",
    "    # Color the ticks.\n",
    "    ax.tick_params(axis=\"y\", colors=color, which=\"both\")\n",
    "\n",
    "\n",
    "# Plot vertical lince indicating variance excess.\n",
    "variance = 0.8\n",
    "for cum_evr, title in [(pca_cum_evr, \"PCA\"), (kpca_cum_evr, \"kPCA\")]:\n",
    "    n_pcs = np.where(cum_evr > variance)[0][0]\n",
    "    # Dashed line indicating the threshold.\n",
    "    ax2.axvline(\n",
    "        n_pcs,\n",
    "        ymin=0,\n",
    "        ymax=1.1,\n",
    "        linestyle=\"dashed\",\n",
    "        color=\"grey\",\n",
    "    )\n",
    "    ax2.text(\n",
    "        1.04 * n_pcs,\n",
    "        0.001,\n",
    "        f\"$N_{{PCs}} = {n_pcs}$ ({title})\",\n",
    "        rotation=90,\n",
    "        color=\"grey\",\n",
    "    )\n",
    "\n",
    "for n_pcs, title in [(6, \"PCA\"), (4, \"kPCA\")]:\n",
    "    # Dashed line indicating the threshold.\n",
    "    ax2.axvline(\n",
    "        n_pcs,\n",
    "        ymin=0,\n",
    "        ymax=1.1,\n",
    "        linestyle=\"-.\",\n",
    "        color=\"grey\",\n",
    "    )\n",
    "    ax2.text(\n",
    "        1.04 * n_pcs,\n",
    "        0.001,\n",
    "        f\"$N_{{PCs}} = {n_pcs}$ ({title})\",\n",
    "        rotation=90,\n",
    "        color=\"grey\",\n",
    "    )\n",
    "ax2.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(plots / \"explained-variance-pca-kpca.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9bc2d2-5cba-46cd-b050-4e37d68c40bf",
   "metadata": {},
   "source": [
    "## LSWR Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316179e-16c7-46c8-a259-da5cb3711849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_abundance(\n",
    "    assignments: list[xr.DataArray],\n",
    ") -> None:\n",
    "    n_subplots = len(assignments)\n",
    "    labels = np.arange(\n",
    "        int(assignments[0].min()), int(assignments[0].max()) + 1, 1, dtype=int\n",
    "    )\n",
    "    x_lims = labels.min() - 0.5, labels.max() + 0.5\n",
    "    bins = np.arange(x_lims[0], x_lims[1] + 1.0, 1.0)\n",
    "    colors = _colors.create_colors_for_labels(labels)\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        figsize=(6, 2 * n_subplots),\n",
    "        nrows=n_subplots,\n",
    "        ncols=1,\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Abundance of LSWRs\")\n",
    "\n",
    "    for i, element in enumerate(assignments):\n",
    "        _, _, patches = axs[i].hist(element, bins=bins, density=True)\n",
    "\n",
    "        axs[i].set_title(element.name)\n",
    "\n",
    "        if i == n_subplots - 1:\n",
    "            axs[i].set_xlabel(\"LSWR\")\n",
    "\n",
    "        axs[i].set_ylabel(\"Abundance [%]\")\n",
    "        axs[i].set_xlim(*x_lims)\n",
    "        axs[i].set_xticks(labels)\n",
    "        axs[i].set_xticklabels(labels, rotation=90)\n",
    "        axs[i].yaxis.grid(True)\n",
    "\n",
    "        # ax2 = axs[i].twinx()\n",
    "        # _, _, patches = ax2.hist(element, bins=bins, density=True)\n",
    "        # ax2.set_ylabel(\"Relative [%]\")\n",
    "\n",
    "        if len(colors) != len(patches):\n",
    "            raise RuntimeError(\n",
    "                \"Length of colors does not match number of patches in histogram\"\n",
    "            )\n",
    "\n",
    "        for color, patch in zip(colors, patches):\n",
    "            patch.set_facecolor(color)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(plots / \"lswrs-abundance-comparison.pdf\")\n",
    "\n",
    "\n",
    "plot_abundance(lswrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93511d32-db51-497b-ba92-97b0cc336ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "\n",
    "def plot_transition_matrix_heatmap(\n",
    "    assignments: list[xr.DataArray],\n",
    ") -> None:\n",
    "    n_subplots = len(assignments)\n",
    "    width_ratios = [1 for _ in enumerate(assignments)] + [0.08]\n",
    "\n",
    "    # Add 1 subplot just for the colorbar\n",
    "    fig, axs = plt.subplots(\n",
    "        figsize=(6 * n_subplots, 6),\n",
    "        nrows=1,\n",
    "        ncols=n_subplots + 1,\n",
    "        gridspec_kw={\"width_ratios\": width_ratios},\n",
    "    )\n",
    "\n",
    "    # Last element of axs is the cbar\n",
    "    for ax1, ax2 in itertools.pairwise(axs[:-1]):\n",
    "        ax1.sharey(ax2)\n",
    "\n",
    "    axs_cbar = axs[-1]\n",
    "    transitions = np.array(\n",
    "        [\n",
    "            a6.plotting.transitions._calculate_markov_transition_matrix(a)\n",
    "            for a in assignments\n",
    "        ]\n",
    "    )\n",
    "    max_prob = transitions.max()\n",
    "    min_prob = transitions.min()\n",
    "\n",
    "    for i, element in enumerate(assignments):\n",
    "        transition = transitions[i]\n",
    "        kwargs = (\n",
    "            {\"cbar\": False}\n",
    "            if i < n_subplots - 1\n",
    "            else {\"cbar_ax\": axs_cbar, \"cbar_kws\": {\"label\": \"probability\"}}\n",
    "        )\n",
    "        sns.heatmap(\n",
    "            transition,\n",
    "            ax=axs[i],\n",
    "            cmap=\"Reds\",\n",
    "            vmin=min_prob,\n",
    "            vmax=max_prob,\n",
    "            **kwargs\n",
    "        )\n",
    "        axs[i].set_title(element.name)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(plots / \"lswrs-transition-probabilities-comparison.pdf\")\n",
    "\n",
    "\n",
    "plot_transition_matrix_heatmap(lswrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a003b-5c3f-402d-ab97-09c0ff72f345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adjacent_values(vals, q1, q3):\n",
    "    upper_adjacent_value = q3 + (q3 - q1) * 1.5\n",
    "    upper_adjacent_value = np.clip(upper_adjacent_value, q3, vals[-1])\n",
    "\n",
    "    lower_adjacent_value = q1 - (q3 - q1) * 1.5\n",
    "    lower_adjacent_value = np.clip(lower_adjacent_value, vals[0], q1)\n",
    "    return lower_adjacent_value, upper_adjacent_value\n",
    "\n",
    "\n",
    "def convert(x):\n",
    "    return x.total_seconds() / 60 / 60 / 24\n",
    "\n",
    "\n",
    "def plot_modes_durations(\n",
    "    assignments: list[xr.DataArray],\n",
    ") -> tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"Plot the mode mean durations and standard deviation.\"\"\"\n",
    "    n_subplots = len(assignments)\n",
    "    modes = [\n",
    "        a6.modes.methods.determine_lifetimes_of_modes(a) for a in assignments\n",
    "    ]\n",
    "    labels = np.arange(1, modes[0].size + 1, 1, dtype=int)\n",
    "\n",
    "    x_lims = labels.min() - 0.5, labels.max() + 0.5\n",
    "    colors = _colors.create_colors_for_labels(labels)\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        figsize=(6, 2 * n_subplots),\n",
    "        nrows=n_subplots,\n",
    "        ncols=1,\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "\n",
    "    for i, element in enumerate(modes):\n",
    "        name = assignments[i].name\n",
    "        print(name)\n",
    "        # (\n",
    "        #     durations,\n",
    "        #     stds,\n",
    "        # ) = a6.plotting.modes.statistics._calculate_mean_durations_and_standard_deviations(\n",
    "        #     element\n",
    "        # )\n",
    "        # axs[i].bar(\n",
    "        #     labels,\n",
    "        #     durations,\n",
    "        #     yerr=stds,\n",
    "        #     width=1.0,  # removes gaps between the bars\n",
    "        #     color=colors,\n",
    "        #     align=\"center\",\n",
    "        #     alpha=1,\n",
    "        #     ecolor=\"black\",\n",
    "        #     capsize=3,\n",
    "        # )\n",
    "\n",
    "        durations = [\n",
    "            [appearance.duration.days for appearance in e.appearances]\n",
    "            for e in element\n",
    "        ]\n",
    "        # parts = axs[i].boxplot(\n",
    "        #     durations,\n",
    "        #     showmeans=True,\n",
    "        #     showfliers=False,\n",
    "        #     showcaps=False,\n",
    "        # )\n",
    "        parts = axs[i].violinplot(\n",
    "            durations,\n",
    "            widths=1,\n",
    "            showmeans=True,\n",
    "            showmedians=True,\n",
    "            showextrema=False,\n",
    "            quantiles=[[0.95] for _ in enumerate(durations)],\n",
    "        )\n",
    "\n",
    "        parts[\"cmeans\"].set_color(\"red\")\n",
    "        parts[\"cmedians\"].set_color(\"orange\")\n",
    "        # parts[\"cmins\"].set_alpha(0.0)\n",
    "        parts[\"cquantiles\"].set_color(\"blue\")\n",
    "\n",
    "        for color, pc in zip(colors, parts[\"bodies\"]):\n",
    "            pc.set_facecolor(\"black\")\n",
    "            # pc.set_edgecolor(color)\n",
    "            pc.set_alpha(0.5)\n",
    "\n",
    "        axs[i].set_title(name)\n",
    "\n",
    "        if i == n_subplots - 1:\n",
    "            axs[i].set_xlabel(\"LSWR\")\n",
    "\n",
    "        axs[i].set_ylabel(\"Duration [days]\")\n",
    "        axs[i].set_xlim(*x_lims)\n",
    "        axs[i].set_xticks(labels)\n",
    "        axs[i].set_xticklabels(labels, rotation=90)\n",
    "        axs[i].yaxis.grid(True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(plots / \"lswrs-durations-comparison.pdf\")\n",
    "\n",
    "\n",
    "plot_modes_durations(lswrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9857305c-36c7-4e71-94a5-e8fd612e9524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def calculate_mean_and_std(data: xr.Dataset, assignments: xr.DataArray):\n",
    "    subset = (\n",
    "        a6.datasets.methods.select.select_levels(levels=500)\n",
    "        >> a6.features.methods.geopotential.calculate_geopotential_height()\n",
    "    ).apply_to(data)\n",
    "\n",
    "    modes = a6.modes.methods.determine_lifetimes_of_modes(assignments)\n",
    "    dates = [list(mode.get_dates()) for mode in modes]\n",
    "    z_h_per_mode = [\n",
    "        subset[\"z_h\"].sel(time=date, method=\"nearest\") for date in dates\n",
    "    ]\n",
    "\n",
    "    means = [z_h.mean(\"time\") for z_h in z_h_per_mode]\n",
    "    means_max = max(mean.max() for mean in means)\n",
    "    means_min = min(mean.min() for mean in means)\n",
    "\n",
    "    stds = [z_h.std(\"time\") for z_h in z_h_per_mode]\n",
    "    stds_max = max(std.max() for std in stds)\n",
    "    stds_min = min(std.min() for std in stds)\n",
    "\n",
    "    return (\n",
    "        assignments.name,\n",
    "        modes,\n",
    "        means,\n",
    "        means_min,\n",
    "        means_max,\n",
    "        stds,\n",
    "        stds_min,\n",
    "        stds_max,\n",
    "    )\n",
    "\n",
    "\n",
    "means_stds = [\n",
    "    calculate_mean_and_std(data=ds, assignments=assignments)\n",
    "    for assignments in lswrs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68208700-4f58-43d2-bb4a-1957b24633bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "\n",
    "def round_to_decade(value: xr.DataArray) -> int:\n",
    "    return int(np.round(value.values, -1))\n",
    "\n",
    "\n",
    "def plot_geopotential_mean_and_std(\n",
    "    name: str,\n",
    "    modes,\n",
    "    means: list[xr.DataArray],\n",
    "    means_min: float,\n",
    "    means_max: float,\n",
    "    stds: list[xr.DataArray],\n",
    "    stds_min: float,\n",
    "    stds_max: float,\n",
    ") -> tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"Plot geopotential height contours, temperature and wind speed.\"\"\"\n",
    "    labels = np.arange(1, modes.size + 1, 1, dtype=int)\n",
    "\n",
    "    n_rows = 8\n",
    "    n_cols = 5\n",
    "\n",
    "    if n_rows * n_cols != labels.max():\n",
    "        raise ValueError(\"Number of plots not equal to number of LSWRs\")\n",
    "\n",
    "    height = n_rows * 2\n",
    "    width = n_cols * 2.5\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        figsize=(width, height),\n",
    "        nrows=n_rows,\n",
    "        # Add 2 columns for colorbars\n",
    "        # ncols=n_cols + 2,\n",
    "        ncols=n_cols,\n",
    "        subplot_kw=a6.plotting.coastlines.create_projection(),\n",
    "        # gridspec_kw={\"width_ratios\": [1 for _ in range(n_cols)] + [0.05, 0.05]},\n",
    "    )\n",
    "    levels = list(\n",
    "        range(\n",
    "            round_to_decade(means_min),\n",
    "            round_to_decade(means_max),\n",
    "            5,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        z_h_mean = means[i]\n",
    "        z_h_std = stds[i]\n",
    "        heatmap = z_h_std.plot(\n",
    "            ax=ax,\n",
    "            cmap=\"RdBu\",\n",
    "            vmin=stds_min,\n",
    "            vmax=stds_max,\n",
    "            add_colorbar=False,\n",
    "        )\n",
    "\n",
    "        contours = z_h_mean.plot.contour(\n",
    "            ax=ax,\n",
    "            levels=levels,\n",
    "            cmap=\"cool\",\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            linewidths=1.0,\n",
    "        )\n",
    "        # contours.clabel(inline=True)\n",
    "        #         handles_unfilled, labels = cs_unfilled.legend_elements()\n",
    "\n",
    "        #         ax.legend(handles_filled + handles_unfilled,\n",
    "        #                   [\"range(2-3)\", \"range(3-4)\", \"range(4-6)\", \"3\", \"4\", \"6\"],\n",
    "        #                   ncols=2)\n",
    "        ax.coastlines(alpha=0.5)\n",
    "        # ax.gridlines(draw_labels=[\"left\", \"bottom\"])\n",
    "        ax.set_title(f\"{i + 1}\")\n",
    "    fig.suptitle(name)\n",
    "    # fig.colorbar(heatmap, ax=axs[:,-2], label=r\"$\\mu(z_\\mathrm{500})$ [m]\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(plots / f\"lswrs-geopotential-heights-{name}.pdf\")\n",
    "\n",
    "\n",
    "for mean_std in means_stds:\n",
    "    plot_geopotential_mean_and_std(*mean_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d54b2-0b86-4cf2-b3db-b66be7ff62af",
   "metadata": {},
   "source": [
    "## Clean production data for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bbedac-83a3-4ff1-8d60-b205e42793be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "NOTE: Preprocess turbine data. This takes ~22 min, so prefer loading (cell below)\n",
    "if no changes to the preprocessing are required.\n",
    "\"\"\"\n",
    "\n",
    "paths = list(\n",
    "    pathlib.Path(\"/p/home/jusers/emmerich1/juwels/data/production\").glob(\n",
    "        \"**/*.nc\"\n",
    "    )\n",
    ")\n",
    "print(paths)\n",
    "\n",
    "\n",
    "def remove_outliers(data: xr.Dataset) -> xr.Dataset:\n",
    "    power_rating = float(data.attrs[\"power rating\"].split()[0])\n",
    "    print(power_rating)\n",
    "    # Only use data points where\n",
    "    # - production is lower than power rating\n",
    "    # - production is greater than 0\n",
    "    return a6.datasets.methods.turbine.clean_production_data(\n",
    "        power_rating=power_rating,\n",
    "    ).apply_to(data)\n",
    "\n",
    "\n",
    "# Contains the turbine name and the production\n",
    "turbines: dict[str, xr.Dataset] = {\n",
    "    path.name: remove_outliers(xr.open_dataset(path)) for path in paths\n",
    "}\n",
    "\n",
    "# NOTE: Uncomment to save preprocessed data to disk.\n",
    "\n",
    "for name, data in turbines.items():\n",
    "    data.to_netcdf(\n",
    "        f\"/p/home/jusers/emmerich1/juwels/data/production-cleaned-for-analysis/{name}.nc\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8c5a3-ea04-4998-b8a0-a72134768fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open preprocessed turbine data if available\n",
    "paths = list(\n",
    "    pathlib.Path(\n",
    "        \"/p/home/jusers/emmerich1/juwels/data/production-cleaned-for-analysis\"\n",
    "    ).glob(\"**/*.nc\")\n",
    ")\n",
    "# Contains the turbine name and the production\n",
    "turbines: dict[str, xr.Dataset] = {\n",
    "    path.name: xr.open_dataset(path) for path in paths\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe11bf-a6cd-4a1e-839c-6c1937b09fb1",
   "metadata": {},
   "source": [
    "## Relation of LSWRs to Power Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255dd850-2a2c-4df7-998d-96e1c2812646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import dataclasses\n",
    "import datetime\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class PowerPerMode:\n",
    "    label: int\n",
    "    measurements: list[int] = dataclasses.field(default_factory=list)\n",
    "    sum: list[float] = dataclasses.field(default_factory=list)\n",
    "    mean: list[float] = dataclasses.field(default_factory=list)\n",
    "    std: list[float] = dataclasses.field(default_factory=list)\n",
    "    normalized_mean: list[float] = dataclasses.field(default_factory=list)\n",
    "    normalized_std: list[float] = dataclasses.field(default_factory=list)\n",
    "\n",
    "\n",
    "def get_power_per_lswr(data: xr.Dataset) -> dict[int, PowerPerMode]:\n",
    "    modes = a6.modes.methods.determine_lifetimes_of_modes(data)\n",
    "    dates = [xr.DataArray(list(mode.get_dates())) for mode in modes]\n",
    "\n",
    "    power_per_mode = {\n",
    "        mode.label: PowerPerMode(label=mode.label) for mode in modes\n",
    "    }\n",
    "\n",
    "    for i, (name, turbine) in enumerate(turbines.items()):\n",
    "        print(f\"{data.name}: {i}/{len(turbines)}\", end=\"\\r\")\n",
    "        power_rating = float(turbine.attrs[\"power rating\"].split()[0])\n",
    "\n",
    "        # Resample to daily production and calculate sum, relative mean and std\n",
    "        resampled = turbine[\"production\"].resample({\"time\": \"1d\"}, skipna=True)\n",
    "        daily_sum = resampled.sum(skipna=True)\n",
    "        daily_mean = resampled.mean(skipna=True)\n",
    "        daily_mean_normalized = daily_mean / power_rating\n",
    "        daily_std = resampled.std(skipna=True)\n",
    "        daily_std_normalized = daily_std / power_rating\n",
    "\n",
    "        for mode, date in zip(modes, dates, strict=True):\n",
    "            mode_power = power_per_mode[mode.label]\n",
    "\n",
    "            # Get time steps of production where LSWR appeared\n",
    "            intersection = sorted(set(daily_sum.time.values) & set(date.values))\n",
    "\n",
    "            # if not intersection:\n",
    "            #     print(f\"WARNING: empty intersection for {name} and mode {mode.label}\")\n",
    "            #     continue\n",
    "\n",
    "            # Count number of days that contribute to the results\n",
    "            mode_power.measurements.append(len(intersection))\n",
    "\n",
    "            # Select time steps of LSWR appearance and calculate sum\n",
    "            total = daily_sum.sel(time=intersection)\n",
    "            mode_power.sum.extend(total.values.flatten().tolist())\n",
    "\n",
    "            # Select time steps of LSWR appearance and calculate mean\n",
    "            mean = daily_mean.sel(time=intersection)\n",
    "            mean_normalized = daily_mean_normalized.sel(time=intersection)\n",
    "            mode_power.mean.extend(mean.values.flatten().tolist())\n",
    "            mode_power.normalized_mean.extend(\n",
    "                mean_normalized.values.flatten().tolist()\n",
    "            )\n",
    "\n",
    "            # Select time steps of LSWR appearance and calculate std\n",
    "            std = daily_std.sel(time=intersection)\n",
    "            std_normalized = daily_std_normalized.sel(time=intersection)\n",
    "            mode_power.std.extend(std.values.flatten().tolist())\n",
    "            mode_power.normalized_std.extend(\n",
    "                std_normalized.values.flatten().tolist()\n",
    "            )\n",
    "    return power_per_mode\n",
    "\n",
    "\n",
    "power_per_method = {lswr.name: get_power_per_lswr(lswr) for lswr in lswrs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a36083-08ce-4b97-82ad-bc29f95f1117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "latex_code = []\n",
    "\n",
    "\n",
    "def power_mean_with_std_as_string(stats: list[float]) -> str:\n",
    "    return f\"${np.nanmean(stats) * 100:.2f} \\pm {np.nanstd(stats) * 100:.2f}$\"\n",
    "\n",
    "\n",
    "columns = {\n",
    "    \"$N$\": lambda lswrs: [\n",
    "        np.nansum(results.measurements) for results in lswrs.values()\n",
    "    ],\n",
    "    \"$P_{\\mathrm{total}}$ [kW]\": lambda lswrs: [\n",
    "        f\"{int(np.nansum(results.sum)):d}\" for results in lswrs.values()\n",
    "    ],\n",
    "    \"$P^{\\mathrm{mean}}_{\\mathrm{normalized}}$ [\\%]\": lambda lswrs: [\n",
    "        power_mean_with_std_as_string(results.normalized_mean)\n",
    "        for results in lswrs.values()\n",
    "    ],\n",
    "    \"$P^{\\mathrm{std}}_{\\mathrm{normalized}}$ [\\%]\": lambda lswrs: [\n",
    "        power_mean_with_std_as_string(results.normalized_std)\n",
    "        for results in lswrs.values()\n",
    "    ],\n",
    "}\n",
    "\n",
    "reform = {\n",
    "    (name, column): func(method)\n",
    "    for name, method in power_per_method.items()\n",
    "    for column, func in columns.items()\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(reform)\n",
    "\n",
    "# Add 1 to start indexing at 1 to be conform with LSWR labels\n",
    "df.index += 1\n",
    "\n",
    "code = df.to_latex(\n",
    "    float_format=\"%.2f\",\n",
    "    label=\"production-per-lswr-per-method\",\n",
    "    caption=f\"Power production for the resulting LSWRs.\",\n",
    ")\n",
    "\n",
    "\n",
    "with open(\n",
    "    \"/p/home/jusers/emmerich1/juwels/code/a6/notebooks/power-production-table.tex\",\n",
    "    \"w\",\n",
    ") as f:\n",
    "    f.write(code)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ad6df-3d27-4981-ad06-7c9e391436a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_power_per_mode(\n",
    "    powers: dict[str, dict[int, PowerPerMode]],\n",
    ") -> tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"Plot the power production for each LSWR and method.\"\"\"\n",
    "    n_rows = len(powers)\n",
    "    n_cols = 1\n",
    "    labels = np.arange(1, n_lswr_categories + 1, dtype=int)\n",
    "\n",
    "    x_lims = labels.min() - 0.5, labels.max() + 0.5\n",
    "    colors = _colors.create_colors_for_labels(labels)\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        figsize=(6 * n_cols, 2 * n_rows),\n",
    "        nrows=n_rows,\n",
    "        ncols=n_cols,\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Power production per LSWRs\")\n",
    "\n",
    "    for i, (method, powers_per_lswr) in enumerate(powers.items()):\n",
    "        powers_means = [\n",
    "            np.nanmean(power.normalized_mean) * 100\n",
    "            for power in powers_per_lswr.values()\n",
    "        ]\n",
    "        powers_stds = [\n",
    "            np.nanstd(power.normalized_mean) * 100\n",
    "            for power in powers_per_lswr.values()\n",
    "        ]\n",
    "        ax = axs[i]\n",
    "\n",
    "        ax.bar(\n",
    "            labels,\n",
    "            powers_means,\n",
    "            yerr=powers_stds,\n",
    "            width=1.0,  # removes gaps between the bars\n",
    "            color=colors,\n",
    "            align=\"center\",\n",
    "            alpha=1,\n",
    "            ecolor=\"black\",\n",
    "            capsize=3,\n",
    "        )\n",
    "        parts = axs[i].violinplot(\n",
    "            durations,\n",
    "            widths=1,\n",
    "            showmeans=True,\n",
    "            showmedians=True,\n",
    "            showextrema=False,\n",
    "            quantiles=[[0.95] for _ in enumerate(durations)],\n",
    "        )\n",
    "\n",
    "        parts[\"cmeans\"].set_color(\"red\")\n",
    "        parts[\"cmedians\"].set_color(\"orange\")\n",
    "        # parts[\"cmins\"].set_alpha(0.0)\n",
    "        parts[\"cquantiles\"].set_color(\"blue\")\n",
    "\n",
    "        for color, pc in zip(colors, parts[\"bodies\"]):\n",
    "            pc.set_facecolor(\"black\")\n",
    "            # pc.set_edgecolor(color)\n",
    "            pc.set_alpha(0.5)\n",
    "\n",
    "        ax.set_title(method)\n",
    "\n",
    "        if i == n_rows - 1:\n",
    "            ax.set_xlabel(\"LSWR\")\n",
    "\n",
    "        ax.set_ylabel(r\"$P^{\\mathrm{mean}}_{\\mathrm{normalized}}$ [%]\")\n",
    "        ax.set_xlim(*x_lims)\n",
    "        ax.set_xticks(labels)\n",
    "        ax.set_xticklabels(labels, rotation=90)\n",
    "        ax.yaxis.grid(True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(plots / \"lswrs-power-production-comparison.pdf\")\n",
    "\n",
    "\n",
    "plot_power_per_mode(power_per_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c112acb-628e-449b-ac04-1e16fefe2b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sses = joblib.load(plots / \"../data/scree-test-results.dict\")\n",
    "sse_pca_1, sse_kpca_1 = sses[\"sses_pca_6_kpca_4\"]\n",
    "sse_pca_2, sse_kpca_2 = sses[\"sses_pca_80_kpca_15\"]\n",
    "\n",
    "ax1_color = \"blue\"\n",
    "ax2_color = \"green\"\n",
    "\n",
    "ks = range(1, len(sse_pca_1) + 1)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(\n",
    "    ks,\n",
    "    sse_pca_1,\n",
    "    linestyle=\"-\",\n",
    "    marker=\"o\",\n",
    "    color=ax1_color,\n",
    "    label=r\"PCA $(N_{\\mathrm{PCs}} = 6)$\",\n",
    ")\n",
    "ax1.plot(\n",
    "    ks,\n",
    "    sse_kpca_1,\n",
    "    linestyle=\"--\",\n",
    "    marker=\"x\",\n",
    "    color=ax1_color,\n",
    "    label=\"kPCA $(N_{\\mathrm{PCs}} = 4)$\",\n",
    ")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    ks,\n",
    "    sse_pca_2,\n",
    "    linestyle=\"-\",\n",
    "    marker=\"o\",\n",
    "    color=ax2_color,\n",
    "    label=r\"PCA $(N_{\\mathrm{PCs}} = 80)$\",\n",
    ")\n",
    "ax2.plot(\n",
    "    ks,\n",
    "    sse_kpca_2,\n",
    "    linestyle=\"--\",\n",
    "    marker=\"x\",\n",
    "    color=ax2_color,\n",
    "    label=\"kPCA $(N_{\\mathrm{PCs}} = 15)$\",\n",
    ")\n",
    "\n",
    "ax1.set_xlabel(r\"$k$\")\n",
    "ax1.set_xticks(ks)\n",
    "ax1.set_xticklabels([str(k) for k in ks], rotation=90)\n",
    "ax1.set_ylabel(\"Sum of squared distance\")\n",
    "\n",
    "for ax, color in [(ax1, ax1_color), (ax2, ax2_color)]:\n",
    "    # Set log scale.\n",
    "    # ax.set(yscale=\"log\")\n",
    "    # Set left xlim such that the first tick disappears.\n",
    "    ax.set_xlim(0.5, max(ks) + 0.5)\n",
    "    # Color the ticks.\n",
    "    ax.tick_params(axis=\"y\", colors=color, which=\"both\")\n",
    "\n",
    "fig.legend(bbox_to_anchor=(1.25, 0.95))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(plots / \"kmeans-scree-test.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fbb21e-4cb8-4533-bcda-92d711d3da94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a6-cuda",
   "language": "python",
   "name": "a6-cuda"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
