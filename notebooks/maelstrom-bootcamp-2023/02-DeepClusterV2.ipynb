{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4708695-e8ba-4d8c-9f03-892f856ef66b",
   "metadata": {},
   "source": [
    "# DeepClusterV2\n",
    "\n",
    "In the first notebook, you had a first insight into xarray and the data set we're going to use.\n",
    "\n",
    "The ERA5 data originally have a temporal resolution of 1 hour on a $30\\,\\mathrm{km}$ grid (0.25$^\\circ$). Our data set contains data on multiple pressure levels $300, 500, 700, 850, 950\\,\\mathrm{hPa}$. Although the model provides a large variety of physical output quantities, we here though only make use of five:\n",
    "\n",
    "1. Geopotential height $z$ $[10^{-2}\\,]$\n",
    "1. Temperature $t$ $[\\mathrm{K}]$\n",
    "1. Relative humidity $r$ $[\\%]$\n",
    "1. Azimuthal wind speed $u$ $[\\mathrm{m}/\\mathrm{s}]$\n",
    "1. Vertical wind speed $v$ $[\\mathrm{m}/\\mathrm{s}]$\n",
    "\n",
    "The coordinates used are \n",
    "\n",
    "1. Time\n",
    "1. Pressure Level\n",
    "1. Latitude\n",
    "1. Longitude\n",
    "\n",
    "The order of these follows the [CF 1.6 conventions for NetCDF data](https://cfconventions.org/cf-conventions/v1.6.0/cf-conventions.html#dimensions), which is also given in the datasets metadata.\n",
    "\n",
    "As you can see from your plots made in the previous notebook, the data cover the whole of Europe. As stated above, the data is given at a resolution of $\\sim 30\\,\\mathrm{km}$.\n",
    "Plotting the temperature field nicely enables you to identify and distinguish the continental oceanic areas.\n",
    "\n",
    "## The problem were trying to solve\n",
    "\n",
    "The goal of Application is to identify recurring large-scale weather patterns (or regimes, _LSWRs_) over Europe and eventually investigate whether their occurrence has an effect on the power production forecast quality of ML models.\n",
    "\n",
    "We want to find these patterns on a daily basis, meaning we assume that each pattern roughly occurs and lasts for at least one day.\n",
    "\n",
    "So first, we aim to develop an unsupervised clustering algorithm that is able to identify patterns in high-dimensional data: we want to make use of multiple physical quantities on multiple pressure levels over the whole of Europe.\n",
    "\n",
    "One proven algorithm that allows unsupervised clustering of images is _DeepClusterV2_ (_DCv2_,see [the DCv2 paper](https://arxiv.org/abs/2006.09882v5)), which achieves high accuracy on typical tasks in image recognition. The procedure of DCv2 is as follows:\n",
    "\n",
    "1. Makes use of different data augmentation strategies (random cropping, rotation, mirroring)\n",
    "2. Feeds to images to a CNN (ResNet50) and an MLP, which has a certain output dimensionality.\n",
    "3. Clusters the images in the low-dimensional feature space that is the output of the CNN+MLP\n",
    "4. Uses the cluster assignments to run backpropagation on the CNN+MLP to adjust the weights such that similar samples get closer and closer with each iteration.\n",
    "\n",
    "\n",
    "<img src=\"./images/dcv2-architecture.png\" width=\"100%\" height=\"100%\">\n",
    "\n",
    "Since DCv2 originally only works with typical RGB images (i.e. 3 input channels), we need to adjust the ResNet to allow more than 3 input channels, depending on how many pressure levels and variables per level we want to use for training.\n",
    "\n",
    "Each sample will represent one day in our time series of ERA5 data, and we will use multiple levels and variables.\n",
    "\n",
    "**Question:** If we use 2 pressure levels, e.g. $500\\,\\mathrm{hPa}$ and $950\\,\\mathrm{hPa}$, and the variables $z$, $r$, and $T$, how many input channels would the ResNet require?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b58d72c",
   "metadata": {},
   "source": [
    "## Now Let's Code\n",
    "\n",
    "We will now take a look at the required code. First, we will import all required modules and set up a logger that will allow us to print any important information from anywhere in our code to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec5ac82-15e4-4d80-98bb-afc673f9c19b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import scipy.constants as constants\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xarray as xr\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import openTSNE\n",
    "\n",
    "import a6\n",
    "import a6.dcv2._logs as logs\n",
    "import a6.dcv2._averaging as averaging\n",
    "import a6.plotting._colors as _colors\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "logger = logging.getLogger(\"notebook\")\n",
    "logger.info(\"Logger initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21445227",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "For model training, we will be using PyTorch. For image recognition, PyTorch provides an additional packaged called `torchvision` that provides a lot of extremely helpful methods and features\n",
    "for image processing. One example is common transformations of images to achieve data augmentation. It also provides a set of so-called data loaders that allow to load common data types for\n",
    "different ML tasks.\n",
    "\n",
    "For supervised learning, for example, it provides a data loader that allows to reach images (e.g. from TIF format) and respective labels of each image.\n",
    "However, we neither do have labels nor is our data in TIF format since we simply don't use images.\n",
    "\n",
    "As a consequence, we have to write our own data loader. Luckily, torchvision data loaders have a very lightweight abstract interface that allows to implement data loaders for any kind of data. For more details, see the [torchvision docs](https://pytorch.org/vision/main/datasets.html).\n",
    "\n",
    "Furthermore, we will do _some_ preprocessing of the data before feeding them to the ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63874afa-4a9d-4985-a73e-783ffe6dc4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@a6.utils.make_functional\n",
    "def calculate_geopotential_height(\n",
    "    data: xr.Dataset,\n",
    "    scaling: float = 10.0,\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Calculate the geopotential height from the geopotential.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    data : xr.Dataset\n",
    "        Data containing the geopotential.\n",
    "    scaling : float, default=10.0\n",
    "        Parameter used for scaling the data.\n",
    "        E.g. the ERA5 geopotential is given in decameters.\n",
    "\n",
    "    \"\"\"\n",
    "    data[\"z_h\"] = data[\"z\"] / constants.g / scaling\n",
    "    return data\n",
    "\n",
    "\n",
    "@a6.utils.make_functional\n",
    "def drop_variables_from_dataset(\n",
    "    data: xr.Dataset,\n",
    "    names: str | list[str],\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Drop variables from dataset.\"\"\"\n",
    "    return data.drop_vars(names)\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    path: pathlib.Path,\n",
    "    is_netcdf: bool,\n",
    "    nmb_crops: tuple[int, ...],\n",
    "    size_crops: tuple[float, ...],\n",
    "    min_scale_crops: tuple[float, ...],\n",
    "    max_scale_crops: tuple[float, ...],\n",
    "    drop_variables: list[str] | None = None,\n",
    ") -> a6.datasets.crop.Base:\n",
    "    if is_netcdf:\n",
    "        logger.info(\"Reading data from netCDF files %s\", path)\n",
    "        ds = xr.open_dataset(\n",
    "            path,\n",
    "            engine=\"netcdf4\",\n",
    "            drop_variables=drop_variables or [],\n",
    "        )\n",
    "        postprocessing = (\n",
    "            a6.features.methods.weighting.weight_by_latitudes(\n",
    "                latitudes=\"latitude\",\n",
    "                use_sqrt=True,\n",
    "            )\n",
    "            >> calculate_geopotential_height()\n",
    "            >> drop_variables_from_dataset(names=[\"z\"])\n",
    "        )\n",
    "        logger.info(\"Applying postprocessing to dataset\")\n",
    "        ds = postprocessing(ds)\n",
    "        return a6.datasets.crop.MultiCropXarrayDataset(\n",
    "            data_path=path,\n",
    "            dataset=ds,\n",
    "            nmb_crops=nmb_crops,\n",
    "            size_crops=size_crops,\n",
    "            min_scale_crops=min_scale_crops,\n",
    "            max_scale_crops=max_scale_crops,\n",
    "            return_index=True,\n",
    "        )\n",
    "    logger.info(\"Assuming image folder dataset\")\n",
    "    return a6.datasets.crop.MultiCropDataset(\n",
    "        data_path=path,\n",
    "        nmb_crops=nmb_crops,\n",
    "        size_crops=size_crops,\n",
    "        min_scale_crops=min_scale_crops,\n",
    "        max_scale_crops=max_scale_crops,\n",
    "        return_index=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ce113",
   "metadata": {},
   "source": [
    "Now we can load the data and define how many augmented crops we want to train our model with, and what the parameters for these crops are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa07a525",
   "metadata": {},
   "source": [
    "## The Clustering Algorithm\n",
    "\n",
    "The following code contains the core part of the algorithm that performs the spherical $K$-Means on our data in feature space.\n",
    "\n",
    "The `init_embeddings` method initially runs our data $X_i$ through the ResNet and MLP and stores the resulting feature space vectors $Z_i$ of each sample (in the code, these are called `embeddings`).\n",
    "The `indexes` tensor stores the indexes of each sample in our original data set. An index of `10`, for example, reflects the 10th day from the beginning of our data set.\n",
    "\n",
    "The `cluster_embeddings` method then uses the embeddings of the previous epoch (or the initial embeddings in case of the first epoch), and clusters them with a spherical $K$-Means.\n",
    "To do so, it randomly selects some samples from the data set as centroids and then assigns each sample to these centroids depending on their distance to each centroid.\n",
    "I.e., spherical $K$-Means assigns each sample the cluster label of the cluster centroid whose positional $128$-D vector is closest to the positional vector of the sample.\n",
    "\n",
    "Here, we store also the sample indexes, embeddings, the cluster assignments, and the indexes of the centroid samples to torch tensors on the disk, and we create some plots to illustrate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c47d33-7d0f-4290-ad46-5089141421f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_embeddings(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: nn.Module,\n",
    "    device: torch.device,\n",
    "    feature_dimensions: int,\n",
    "    crops_for_assign: tuple[int, float],\n",
    "    drop_last: bool = False,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    size_dataset = len(dataloader.dataset)\n",
    "    logger.info(\"Dataset size is %i samples\", size_dataset)\n",
    "\n",
    "    if drop_last:\n",
    "        size_dataset -= size_dataset % settings.model.batch_size\n",
    "        logger.warning(\n",
    "            \"Adjusted size of memory per process due to drop_last=True to %i\",\n",
    "            size_dataset,\n",
    "        )\n",
    "\n",
    "    logger.info(\"Processing %i samples in total\", size_dataset)\n",
    "\n",
    "    indexes = torch.zeros(size_dataset).long().to(device=device)\n",
    "    embeddings = torch.zeros(\n",
    "        len(crops_for_assign),\n",
    "        size_dataset,\n",
    "        feature_dimensions,\n",
    "    ).to(device=device)\n",
    "\n",
    "    start_idx = 0\n",
    "    with torch.no_grad():\n",
    "        logger.info(\"Start initializing the embeddings\")\n",
    "        for index, inputs in dataloader:\n",
    "            logger.info(\n",
    "                \"Processing %i samples to initialize embeddings\", index.size(0)\n",
    "            )\n",
    "            n_indexes = inputs[0].size(0)\n",
    "            index = index.to(device=device, non_blocking=True)\n",
    "\n",
    "            # get embeddings\n",
    "            outputs = []\n",
    "            for crop_idx in crops_for_assign:\n",
    "                inp = inputs[crop_idx].to(device=device, non_blocking=True)\n",
    "                outputs.append(model(inp)[0])\n",
    "\n",
    "            # fill the memory bank\n",
    "            indexes[start_idx : start_idx + n_indexes] = index\n",
    "            for mb_idx, embedding in enumerate(outputs):\n",
    "                embeddings[mb_idx][\n",
    "                    start_idx : start_idx + n_indexes\n",
    "                ] = embedding\n",
    "            start_idx += n_indexes\n",
    "    logger.info(\n",
    "        \"Initialization of embeddings done with %s indexes\",\n",
    "        indexes.size(),\n",
    "    )\n",
    "    return indexes, embeddings\n",
    "\n",
    "\n",
    "def cluster_embeddings(\n",
    "    epoch: int,\n",
    "    model,\n",
    "    indexes: torch.Tensor,\n",
    "    embeddings: torch.Tensor,\n",
    "    size_dataset: int,\n",
    "    device: torch.device,\n",
    "    crops_for_assign: tuple[float, ...],\n",
    "    nmb_prototypes: tuple[int, ...],\n",
    "    feature_dimensions: int,\n",
    "    n_epochs: int,\n",
    "    nmb_kmeans_iters: int,\n",
    "    plots_path: pathlib.Path = pathlib.Path(\".\"),\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    logger.info(\"Clustering %i samples\", size_dataset)\n",
    "\n",
    "    # j defines which crops are used for the K-means run.\n",
    "    # E.g. if the number of crops (``self.nmb_mbs``) is 2, and\n",
    "    # ``self.num_clusters = [30, 30, 30, 30]``, the crops will\n",
    "    # be used as following:\n",
    "    #\n",
    "    # 1. K=30, j=0\n",
    "    # 2. K=30, j=1\n",
    "    # 3. K=30, j=0\n",
    "    # 4. K=30, j=1\n",
    "    j = 0\n",
    "\n",
    "    n_heads = len(nmb_prototypes)\n",
    "    n_clusters = nmb_prototypes[0]\n",
    "\n",
    "    assignments_per_prototype = (torch.zeros(n_heads, size_dataset).long()).to(\n",
    "        device\n",
    "    )\n",
    "    indexes_per_prototype = torch.zeros(n_heads, size_dataset).long().to(device)\n",
    "    centroids_indexes_per_prototype = torch.zeros(n_heads, n_clusters).to(\n",
    "        device\n",
    "    )\n",
    "\n",
    "    embeddings_per_prototype = torch.zeros(\n",
    "        n_heads,\n",
    "        *tuple(embeddings.size()),\n",
    "    ).to(device)\n",
    "    distances_per_prototype = torch.zeros(n_heads, size_dataset).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i_K, K in enumerate(nmb_prototypes):\n",
    "            # run k-means\n",
    "\n",
    "            # init with random samples as centroids from the dataset\n",
    "            centroids = torch.empty(K, feature_dimensions).to(\n",
    "                device=device, non_blocking=True\n",
    "            )\n",
    "\n",
    "            batch_size = len(embeddings[j])\n",
    "            random_idx = torch.randperm(batch_size)[:K]\n",
    "            assert len(random_idx) >= K, (\n",
    "                f\"Please reduce the number of centroids K={K}: \"\n",
    "                f\"K must be smaller than batch size {batch_size}\"\n",
    "            )\n",
    "            centroids = embeddings[j][random_idx]\n",
    "\n",
    "            for n_iter in range(nmb_kmeans_iters + 1):\n",
    "                # E step\n",
    "                dot_products = torch.mm(embeddings[j], centroids.t())\n",
    "                distances, assignments = dot_products.max(dim=1)\n",
    "\n",
    "                # finish\n",
    "                if n_iter == nmb_kmeans_iters:\n",
    "                    break\n",
    "\n",
    "                # M step\n",
    "                where_helper = _get_indices_sparse(assignments.cpu().numpy())\n",
    "                counts = (\n",
    "                    torch.zeros(K).to(device=device, non_blocking=True).int()\n",
    "                )\n",
    "                emb_sums = torch.zeros(K, feature_dimensions).to(\n",
    "                    device=device, non_blocking=True\n",
    "                )\n",
    "                for k in range(len(where_helper)):\n",
    "                    if len(where_helper[k][0]) > 0:\n",
    "                        emb_sums[k] = torch.sum(\n",
    "                            embeddings[j][where_helper[k][0]],\n",
    "                            dim=0,\n",
    "                        )\n",
    "                        counts[k] = len(where_helper[k][0])\n",
    "                mask = counts > 0\n",
    "                centroids[mask] = emb_sums[mask] / counts[mask].unsqueeze(1)\n",
    "\n",
    "                # normalize centroids\n",
    "                centroids = nn.functional.normalize(centroids, dim=1, p=2)\n",
    "\n",
    "            # Copy centroids to model for forwarding\n",
    "            getattr(\n",
    "                model.prototypes,\n",
    "                \"prototypes\" + str(i_K),\n",
    "            ).weight.copy_(centroids)\n",
    "\n",
    "            logger.info(\"embeddings[j=%i]: %s\", j, embeddings[j].size())\n",
    "\n",
    "            # Save results to local tensors\n",
    "            assignments_per_prototype[i_K][indexes] = assignments\n",
    "            indexes_per_prototype[i_K][indexes] = indexes\n",
    "            centroids_indexes_per_prototype[i_K] = random_idx\n",
    "            # For the embeddings, make sure to use j for indexing\n",
    "            embeddings_per_prototype[i_K][j][indexes] = embeddings[j]\n",
    "\n",
    "            # next memory bank to use\n",
    "            j = (j + 1) % len(crops_for_assign)\n",
    "\n",
    "    return assignments_per_prototype, centroids_indexes_per_prototype\n",
    "\n",
    "\n",
    "def _get_indices_sparse(data):\n",
    "    cols = np.arange(data.size)\n",
    "    M = csr_matrix(\n",
    "        (cols, (data.ravel(), cols)), shape=(int(data.max()) + 1, data.size)\n",
    "    )\n",
    "    return [np.unravel_index(row.data, data.shape) for row in M]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f062c7a",
   "metadata": {},
   "source": [
    "## The Training Step\n",
    "\n",
    "The `train` method trains our model (i.e. runs the samples through the CNN+MLP and performs the $K$-Means) and then computes the loss to eventually adjust the model weights via backpropagation.\n",
    "\n",
    "The loss here is defined by the difference in cluster assignments between the different random crops of the same sample.\n",
    "\n",
    "For example: the same sample (day in our time series) results in two random crops. If the ResNet+MLP+$K$-Means then assigns different labels to both crops, it gets \"punished\" because this increases the loss.\n",
    "On the other hand, if both crops get the same cluster label - which is desirable - this will positively affect the loss and hence the backpropagation.\n",
    "Eventually, this will push our model to bring similar samples closer to each other: their positions in feature space will evolve to eventually have clusters of similar samples.\n",
    "\n",
    "We hope that these clusters reflect similar LSWRs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b8cd8-a1cf-4932-9f46-b98d86597d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    indexes: torch.Tensor,\n",
    "    embeddings: torch.Tensor,\n",
    "    nmb_crops: tuple[float, ...],\n",
    "    crops_for_assign: tuple[float, ...],\n",
    "    nmb_prototypes: tuple[int, ...],\n",
    "    feature_dimensions: int,\n",
    "    n_epochs: int,\n",
    "    nmb_kmeans_iters: int,\n",
    "    temperature: float,\n",
    "    device: torch.device,\n",
    ") -> tuple[tuple[int, float], torch.Tensor, torch.Tensor]:\n",
    "    batch_time = averaging.AverageMeter()\n",
    "    data_time = averaging.AverageMeter()\n",
    "    losses = averaging.AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    assignments, centroids_indexes = cluster_embeddings(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        indexes=indexes,\n",
    "        embeddings=embeddings,\n",
    "        size_dataset=len(dataloader.dataset),\n",
    "        device=device,\n",
    "        crops_for_assign=crops_for_assign,\n",
    "        nmb_prototypes=nmb_prototypes,\n",
    "        feature_dimensions=feature_dimensions,\n",
    "        nmb_kmeans_iters=nmb_kmeans_iters,\n",
    "        n_epochs=n_epochs,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Clustering for epoch %i done\", epoch)\n",
    "\n",
    "    end = time.time()\n",
    "    start_idx = 0\n",
    "    for it, (idx, inputs) in enumerate(dataloader):\n",
    "        logger.debug(\"Calculating loss for index %s\", idx)\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # ============ multi-res forward passes ... ============\n",
    "        # Output here returns the output for each head (prototype)\n",
    "        # and hence has size ``len(settings.model.nmb_prototypes)``.\n",
    "        emb, output = model(inputs)\n",
    "        emb = emb.detach()\n",
    "        bs = inputs[0].size(0)\n",
    "\n",
    "        if bs == 0:\n",
    "            raise RuntimeError(\n",
    "                f\"Batch size is zero, loss will be NaN: it={it}, idx={idx}, \"\n",
    "                \"inputs[0]={inputs[0]}\"\n",
    "            )\n",
    "\n",
    "        # ============ deepcluster-v2 loss ... ============\n",
    "        loss = torch.tensor(0.0).to(device=device)\n",
    "        for h in range(len(nmb_prototypes)):\n",
    "            scores = output[h] / temperature\n",
    "            targets = (\n",
    "                assignments[h][idx]\n",
    "                .repeat(sum(nmb_crops))\n",
    "                .to(device=device, non_blocking=True)\n",
    "            )\n",
    "            loss += cross_entropy(scores, targets)\n",
    "        loss /= len(nmb_prototypes)\n",
    "\n",
    "        # ============ backward and optim step ... ============\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ============ update memory banks ... ============\n",
    "        indexes[start_idx : start_idx + bs] = idx\n",
    "        for i, crop_idx in enumerate(crops_for_assign):\n",
    "            embeddings[i][start_idx : start_idx + bs] = emb[  # noqa: E203\n",
    "                crop_idx * bs : (crop_idx + 1) * bs\n",
    "            ]\n",
    "        start_idx += bs\n",
    "\n",
    "        # ============ misc ... ============\n",
    "        losses.update(loss.item(), bs)\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if it % 50 == 0:\n",
    "            logger.info(\n",
    "                \"[EPOCH %i, ITERATION %i] \"\n",
    "                \"batch time: %s (%s) \"\n",
    "                \"data load time: %s (%s) \"\n",
    "                \"loss: %s (%s) \"\n",
    "                \"lr: %s\",\n",
    "                epoch,\n",
    "                it,\n",
    "                batch_time.val,\n",
    "                batch_time.avg,\n",
    "                data_time.val,\n",
    "                data_time.avg,\n",
    "                losses.val,\n",
    "                losses.avg,\n",
    "                optimizer.state_dict()[\"param_groups\"][0][\"lr\"],\n",
    "            )\n",
    "\n",
    "    logger.debug(\n",
    "        \"========= Memory Summary at epoch %i =======\\n%s\\n\",\n",
    "        epoch,\n",
    "        torch.cuda.memory_summary(),\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        (epoch, losses.avg),\n",
    "        indexes,\n",
    "        embeddings,\n",
    "        assignments,\n",
    "        centroids_indexes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e6172",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "\n",
    "The `start` method is the \"entry point\" of our training: here we initialize the data loader, model and optimizer (Stochastic Gradient Descent).\n",
    "\n",
    "Then, we perform an initial forwarding of our samples through the model via `init_embeddings` and then perform the training step (`train` method)\n",
    "for the desired number of epochs.\n",
    "\n",
    "Meanwhile, we track our training progress, most importantly the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f711d-4344-4704-bdf4-89c979e6bdb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start(\n",
    "    train_dataset: a6.datasets.crop.Base,\n",
    "    epochs: int,\n",
    "    nmb_crops: tuple[float, ...],\n",
    "    crops_for_assign: tuple[float, ...],\n",
    "    model_architecture: a6.models.resnet.Architecture = a6.models.resnet.Architecture.ResNet50,\n",
    "    batch_size: int = 64,\n",
    "    drop_last: bool = False,\n",
    "    hidden_mlp: int = 2048,\n",
    "    feature_dimensions: int = 128,\n",
    "    nmb_kmeans_iters: int = 10,\n",
    "    nmb_prototypes: int = 3,\n",
    "    nmb_clusters: int = 40,\n",
    "    base_lr: float = 4.8,\n",
    "    weight_decay: float = 1e-6,\n",
    "    temperature: float = 0.1,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    nmb_prototypes = [nmb_clusters for _ in range(nmb_prototypes)]\n",
    "    logger.info(\"Prototypes for model: %s\", nmb_prototypes)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        # ``drop_last=True`` gives each device the same amount of samples,\n",
    "        # but removes some from the clustering.\n",
    "        drop_last=drop_last,\n",
    "        worker_init_fn=a6.utils.distributed.set_dataloader_seeds,\n",
    "    )\n",
    "    logger.info(\"Building data done with %s images loaded\", len(train_dataset))\n",
    "\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    logger.info(\"Using device %s\", device)\n",
    "\n",
    "    # build model\n",
    "    model = a6.models.resnet.Models[model_architecture](\n",
    "        normalize=True,\n",
    "        in_channels=train_dataset.n_channels,\n",
    "        hidden_mlp=hidden_mlp,\n",
    "        output_dim=feature_dimensions,\n",
    "        nmb_prototypes=nmb_prototypes,\n",
    "        device=device,\n",
    "    )\n",
    "    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "\n",
    "    # Copy model to GPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    logger.info(model)\n",
    "    logger.info(\"Building model done\")\n",
    "\n",
    "    # build optimizer\n",
    "    # Should be done after moving the model to GPU\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=base_lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Building optimizer done\")\n",
    "    training_stats = logs.Stats(\"stats.csv\", columns=(\"epoch\", \"loss\"))\n",
    "\n",
    "    indexes, embeddings = init_embeddings(\n",
    "        dataloader=train_loader,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        feature_dimensions=feature_dimensions,\n",
    "        crops_for_assign=crops_for_assign,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # train the network for one epoch\n",
    "        logger.info(\"============ Starting epoch %i ============\", epoch)\n",
    "\n",
    "        # set sampler\n",
    "        # train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        # train the network\n",
    "        scores, indexes, embeddings, assignments, centroids_indexes = train(\n",
    "            dataloader=train_loader,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            n_epochs=epochs,\n",
    "            indexes=indexes,\n",
    "            embeddings=embeddings,\n",
    "            nmb_crops=nmb_crops,\n",
    "            crops_for_assign=crops_for_assign,\n",
    "            nmb_prototypes=nmb_prototypes,\n",
    "            feature_dimensions=feature_dimensions,\n",
    "            nmb_kmeans_iters=nmb_kmeans_iters,\n",
    "            device=device,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        training_stats.update(scores)\n",
    "    return indexes, embeddings, assignments, centroids_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c9ec77-b79f-4e13-8959-5fcbcc26705d",
   "metadata": {},
   "source": [
    "### Load the data set and define the data augmentation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ab815-d936-4856-b0d3-9ed36d5a33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pathlib.Path(\n",
    "    \"/p/project/training2330/a6/data/ecmwf_era5/era5_pl_2017_2020_12.nc\"\n",
    ")\n",
    "nmb_crops = (2,)\n",
    "crops_for_assign = (0, 1)\n",
    "size_crops = (0.75,)\n",
    "min_scale_crops = (0.15,)\n",
    "max_scale_crops = (1.0,)\n",
    "\n",
    "train_dataset = create_dataset(\n",
    "    path=path,\n",
    "    is_netcdf=True,\n",
    "    nmb_crops=nmb_crops,\n",
    "    size_crops=size_crops,\n",
    "    min_scale_crops=min_scale_crops,\n",
    "    max_scale_crops=max_scale_crops,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed7f5a-0a69-4876-b088-ea515a98e88a",
   "metadata": {},
   "source": [
    "### Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717224d3-0ae6-4ee0-a341-0ca0b7b7189c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    indexes,\n",
    "    embeddings_per_crop,\n",
    "    assignments_per_prototype,\n",
    "    centroids_indexes_per_prototype,\n",
    ") = start(\n",
    "    train_dataset=train_dataset,\n",
    "    epochs=10,\n",
    "    nmb_crops=nmb_crops,\n",
    "    crops_for_assign=crops_for_assign,\n",
    "    batch_size=64,\n",
    "    nmb_prototypes=3,\n",
    "    nmb_clusters=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf55f89-5c10-4841-92ee-14e4447aa9fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_embeddings_using_tsne(\n",
    "    embeddings: torch.Tensor,\n",
    "    assignments: torch.Tensor,\n",
    "    centroids: torch.Tensor,\n",
    ") -> None:\n",
    "    \"\"\"Plot the embeddings of DCv2 using t-SNE.\n",
    "\n",
    "    Args:\n",
    "        embeddings (torch.Tensor, shape(n_crops, n_samples, n_embedding_dims)):\n",
    "            Embeddings as produced by the ResNet.\n",
    "        assignments (torch.Tensor, shape(n_samples)):\n",
    "            Assignments by DCv2 for each sample.\n",
    "\n",
    "            Used for coloring each sample in the plot.\n",
    "        centroids (torch.Tensor): The indexes of the centroids.\n",
    "\n",
    "    \"\"\"\n",
    "    logging.info(\"Creating plot for embeddings\")\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "\n",
    "    ax.set_title(f\"Embeddings for crops\")\n",
    "\n",
    "    (x, y), (x_centroids, y_centroids) = _fit_tsne(\n",
    "        embeddings=embeddings, centroids=centroids\n",
    "    )\n",
    "    colors = _colors.create_colors_for_assigments(assignments)\n",
    "\n",
    "    ax.scatter(x, y, c=colors, s=1)\n",
    "    ax.scatter(x_centroids, y_centroids, c=\"red\", s=20, marker=\"x\")\n",
    "\n",
    "\n",
    "def _fit_tsne(\n",
    "    embeddings: torch.Tensor, centroids: torch.Tensor\n",
    ") -> tuple[tuple[tuple[float, float], tuple[float, float]]]:\n",
    "    result = openTSNE.TSNE().fit(embeddings.cpu())\n",
    "    return zip(*result), zip(*result[centroids])\n",
    "\n",
    "\n",
    "assignments_cpu = assignments_per_prototype[-1].cpu()\n",
    "embeddings_cpu = embeddings_per_crop[-1].cpu()\n",
    "centroids_indexes_cpu = centroids_indexes_per_prototype[-1].cpu()\n",
    "plot_embeddings_using_tsne(\n",
    "    embeddings=embeddings_cpu,\n",
    "    assignments=assignments_cpu,\n",
    "    centroids=centroids_indexes_cpu,\n",
    ")\n",
    "a6.plotting.assignments.plot_abundance(assignments_cpu)\n",
    "a6.plotting.transitions.plot_transition_matrix_heatmap(assignments_cpu)\n",
    "a6.plotting.transitions.plot_transition_matrix_clustermap(assignments_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff43c4-6a4b-4b58-9f65-b49d15b5346f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Possible Tasks\n",
    "\n",
    "1. Abundance of LSWRs vs abundance of DWD Großwetterlagen\n",
    "   1. total\n",
    "   2. per month\n",
    "2. Plot meteorological quantities for individual clusters and compare (`a6.plotting.plot_fields_for_dates`).\n",
    "3. Plot meteorological quantities for samples closest to the cluster centroids and compoare (`a6.plotting.plot_fields_for_dates`).\n",
    "4. Vary number of clusters (`n_clusters`) and compare the results.\n",
    "5. Adapt data augmentation strategy and investigate affect on results.\n",
    "6. Relation of forecast error to LSWRs (`a6.studies.grid_search.perform_forecast_model_grid_search`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maelstrom-bootcamp-2023",
   "language": "python",
   "name": "maelstrom-bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
