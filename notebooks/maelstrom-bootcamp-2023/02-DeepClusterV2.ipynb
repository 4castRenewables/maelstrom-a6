{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4708695-e8ba-4d8c-9f03-892f856ef66b",
   "metadata": {},
   "source": [
    "# DeepClusterV2\n",
    "\n",
    "In the first notebook, you had a first insight into xarray and the data set we're going to use.\n",
    "\n",
    "The ERA5 data originally have a temporal resolution of 1 hour on a $30\\,\\mathrm{km}$ grid (0.25$^\\circ$). Our data set contains data on multiple pressure levels $300, 500, 700, 850, 950\\,\\mathrm{hPa}$. Although the model provides a large variety of physical output quantities, we here though only make use of five:\n",
    "\n",
    "1. Geopotential height $z$ $[10^{-2}\\,]$\n",
    "1. Temperature $t$ $[\\mathrm{K}]$\n",
    "1. Relative humidity $r$ $[\\%]$\n",
    "1. Azimuthal wind speed $u$ $[\\mathrm{m}/\\mathrm{s}]$\n",
    "1. Vertical wind speed $v$ $[\\mathrm{m}/\\mathrm{s}]$\n",
    "\n",
    "The coordinates used are \n",
    "\n",
    "1. Time\n",
    "1. Pressure Level\n",
    "1. Latitude\n",
    "1. Longitude\n",
    "\n",
    "The order of these follows the [CF 1.6 conventions for NetCDF data](https://cfconventions.org/cf-conventions/v1.6.0/cf-conventions.html#dimensions), which is also given in the datasets metadata.\n",
    "\n",
    "As you can see from your plots made in the previous notebook, the data cover the whole of Europe. As stated above, the data is given at a resolution of $\\sim 30\\,\\mathrm{km}$.\n",
    "Plotting the temperature field nicely enables you to identify and distinguish the continental oceanic areas.\n",
    "\n",
    "## The problem were trying to solve\n",
    "\n",
    "The goal of Application is to identify recurring weather patterns over Europe and eventually investigate whether their occurrence has an effect on the power production forecast quality of ML models.\n",
    "\n",
    "We want to find these patterns on a daily basis, meaning we assume that each pattern roughly occurs and lasts for at least one day.\n",
    "\n",
    "So first, we aim to develop an unsupervised clustering algorithm that is able to identify patterns in high-dimensional data: we want to make use of multiple physical quantities on multiple pressure levels over the whole of Europe.\n",
    "\n",
    "One proven algorithm that allows unsupervised clustering of images is _DeepClusterV2_ (_DCv2_,see [the DCv2 paper](https://arxiv.org/abs/2006.09882v5)), which achieves high accuracy on typical tasks in image recognition. The procedure of DCv2 is as follows:\n",
    "\n",
    "1. Makes use of different data augmentation strategies (random cropping, rotation, mirroring)\n",
    "2. Feeds to images to a CNN (ResNet50) and an MLP, which has a certain output dimensionality.\n",
    "3. Clusters the images in the low-dimensional feature space that is the output of the CNN+MLP\n",
    "4. Uses the cluster assignments to run backpropagation on the CNN+MLP to adjust the weights such that similar samples get closer and closer with each iteration.\n",
    "\n",
    "\n",
    "\n",
    "Since DCv2 originally only works with typical RGB images (i.e. 3 input channels) We will train a model by constructing samples from each day of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec5ac82-15e4-4d80-98bb-afc673f9c19b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import scipy.constants as constants\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xarray as xr\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import a6\n",
    "import a6.dcv2._logs as logs\n",
    "import a6.dcv2._averaging as averaging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "logger = logging.getLogger(\"notebook\")\n",
    "logger.info(\"Logger initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63874afa-4a9d-4985-a73e-783ffe6dc4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@a6.utils.make_functional\n",
    "def calculate_geopotential_height(\n",
    "    data: xr.Dataset,\n",
    "    scaling: float = 10.0,\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Calculate the geopotential height from the geopotential.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    data : xr.Dataset\n",
    "        Data containing the geopotential.\n",
    "    scaling : float, default=10.0\n",
    "        Parameter used for scaling the data.\n",
    "        E.g. the ERA5 geopotential is given in decameters.\n",
    "\n",
    "    \"\"\"\n",
    "    data[\"z_h\"] = data[\"z\"] / constants.g / scaling\n",
    "    return data\n",
    "\n",
    "\n",
    "@a6.utils.make_functional\n",
    "def drop_variables_from_dataset(\n",
    "    data: xr.Dataset,\n",
    "    names: str | list[str],\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Drop variables from dataset.\"\"\"\n",
    "    return data.drop_vars(names)\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    path: pathlib.Path,\n",
    "    is_netcdf: bool,\n",
    "    nmb_crops: tuple[int, ...],\n",
    "    size_crops: tuple[float, ...],\n",
    "    min_scale_crops: tuple[float, ...],\n",
    "    max_scale_crops: tuple[float, ...],\n",
    "    drop_variables: list[str] | None = None,\n",
    ") -> a6.datasets.crop.Base:\n",
    "    if is_netcdf:\n",
    "        logger.info(\"Reading data from netCDF files %s\", paths)\n",
    "        ds = xr.open_dataset(\n",
    "            path,\n",
    "            engine=\"netcdf4\",\n",
    "            drop_variables=drop_variables or [],\n",
    "        ).isel(time=slice(0, 365))\n",
    "        postprocessing = (\n",
    "            a6.features.methods.weighting.weight_by_latitudes(\n",
    "                latitudes=\"latitude\",\n",
    "                use_sqrt=True,\n",
    "            )\n",
    "            >> calculate_geopotential_height()\n",
    "            >> drop_variables_from_dataset(names=[\"z\"])\n",
    "        )\n",
    "        logger.info(\"Applying postprocessing to dataset\")\n",
    "        ds = postprocessing(ds)\n",
    "        return a6.datasets.crop.MultiCropXarrayDataset(\n",
    "            data_path=path,\n",
    "            dataset=ds,\n",
    "            nmb_crops=nmb_crops,\n",
    "            size_crops=size_crops,\n",
    "            min_scale_crops=min_scale_crops,\n",
    "            max_scale_crops=max_scale_crops,\n",
    "            return_index=True,\n",
    "        )\n",
    "    logger.info(\"Assuming image folder dataset\")\n",
    "    return a6.datasets.crop.MultiCropDataset(\n",
    "        data_path=path,\n",
    "        nmb_crops=nmb_crops,\n",
    "        size_crops=size_crops,\n",
    "        min_scale_crops=min_scale_crops,\n",
    "        max_scale_crops=max_scale_crops,\n",
    "        return_index=True,\n",
    "    )\n",
    "\n",
    "\n",
    "path = pathlib.Path(\n",
    "    \"/p/project/training2330/a6/data/ecmwf_era5/era5_pl_2017_2020_12.nc\"\n",
    ")\n",
    "nmb_crops = (2,)\n",
    "crops_for_assign = (0, 1)\n",
    "size_crops = (0.75,)\n",
    "min_scale_crops = (0.15,)\n",
    "max_scale_crops = (1.0,)\n",
    "\n",
    "train_dataset = create_dataset(\n",
    "    path=path,\n",
    "    is_netcdf=True,\n",
    "    nmb_crops=nmb_crops,\n",
    "    size_crops=size_crops,\n",
    "    min_scale_crops=min_scale_crops,\n",
    "    max_scale_crops=max_scale_crops,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c47d33-7d0f-4290-ad46-5089141421f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_memory(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: nn.Module,\n",
    "    device: torch.device,\n",
    "    feature_dimensions: int,\n",
    "    crops_for_assign: tuple[int, float],\n",
    "    drop_last: bool = False,\n",
    "):\n",
    "    size_dataset = len(dataloader.dataset)\n",
    "    logger.info(\"Dataset size is %i samples\", size_dataset)\n",
    "\n",
    "    if drop_last:\n",
    "        size_dataset -= size_dataset % settings.model.batch_size\n",
    "        logger.warning(\n",
    "            \"Adjusted size of memory per process due to drop_last=True to %i\",\n",
    "            size_dataset,\n",
    "        )\n",
    "\n",
    "    logger.info(\"Processing %i samples\", size_dataset)\n",
    "\n",
    "    indexes = torch.zeros(size_dataset).long().to(device=device)\n",
    "    embeddings = torch.zeros(\n",
    "        len(crops_for_assign),\n",
    "        size_dataset,\n",
    "        feature_dimensions,\n",
    "    ).to(device=device)\n",
    "\n",
    "    start_idx = 0\n",
    "    with torch.no_grad():\n",
    "        logger.info(\"Start initializing the memory banks\")\n",
    "        for index, inputs in dataloader:\n",
    "            logger.info(\n",
    "                \"Processing %i samples from data indexes %s\",\n",
    "                index.size(0),\n",
    "                index,\n",
    "            )\n",
    "            n_indexes = inputs[0].size(0)\n",
    "            index = index.to(device=device, non_blocking=True)\n",
    "\n",
    "            # get embeddings\n",
    "            outputs = []\n",
    "            for crop_idx in crops_for_assign:\n",
    "                inp = inputs[crop_idx].to(device=device, non_blocking=True)\n",
    "                outputs.append(model(inp)[0])\n",
    "\n",
    "            # fill the memory bank\n",
    "            indexes[start_idx : start_idx + n_indexes] = index\n",
    "            for mb_idx, embedding in enumerate(outputs):\n",
    "                embeddings[mb_idx][\n",
    "                    start_idx : start_idx + n_indexes\n",
    "                ] = embedding\n",
    "            start_idx += n_indexes\n",
    "    logger.info(\n",
    "        \"Initialization of the memory banks done with %s local memory indexes\",\n",
    "        indexes.size(),\n",
    "    )\n",
    "    return indexes, embeddings\n",
    "\n",
    "\n",
    "def cluster_memory(\n",
    "    epoch: int,\n",
    "    model,\n",
    "    indexes: torch.Tensor,\n",
    "    embeddings: torch.Tensor,\n",
    "    size_dataset: int,\n",
    "    device: torch.device,\n",
    "    crops_for_assign: tuple[float, ...],\n",
    "    nmb_prototypes: tuple[int, ...],\n",
    "    feature_dimensions: int,\n",
    "    n_epochs: int,\n",
    "    nmb_kmeans_iters: int,\n",
    "    plots_path: pathlib.Path = pathlib.Path(\".\"),\n",
    "):\n",
    "    logger.info(\"Clustering %i samples\", size_dataset)\n",
    "\n",
    "    # j defines which crops are used for the K-means run.\n",
    "    # E.g. if the number of crops (``self.nmb_mbs``) is 2, and\n",
    "    # ``self.num_clusters = [30, 30, 30, 30]``, the crops will\n",
    "    # be used as following:\n",
    "    #\n",
    "    # 1. K=30, j=0\n",
    "    # 2. K=30, j=1\n",
    "    # 3. K=30, j=0\n",
    "    # 4. K=30, j=1\n",
    "    j = 0\n",
    "\n",
    "    n_heads = len(nmb_prototypes)\n",
    "\n",
    "    assignments_per_prototype = (torch.zeros(n_heads, size_dataset).long()).to(\n",
    "        device\n",
    "    )\n",
    "    indexes_per_prototype = torch.zeros(n_heads, size_dataset).long().to(device)\n",
    "\n",
    "    embeddings_per_prototype = torch.zeros(\n",
    "        n_heads,\n",
    "        *tuple(embeddings.size()),\n",
    "    ).to(device)\n",
    "    distances_per_prototype = torch.zeros(n_heads, size_dataset).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i_K, K in enumerate(nmb_prototypes):\n",
    "            # run k-means\n",
    "\n",
    "            # init with random samples as centroids from the dataset\n",
    "            centroids = torch.empty(K, feature_dimensions).to(\n",
    "                device=device, non_blocking=True\n",
    "            )\n",
    "\n",
    "            batch_size = len(embeddings[j])\n",
    "            random_idx = torch.randperm(batch_size)[:K]\n",
    "            assert len(random_idx) >= K, (\n",
    "                f\"Please reduce the number of centroids K={K}: \"\n",
    "                f\"K must be smaller than batch size {batch_size}\"\n",
    "            )\n",
    "            centroids = embeddings[j][random_idx]\n",
    "\n",
    "            for n_iter in range(nmb_kmeans_iters + 1):\n",
    "                # E step\n",
    "                dot_products = torch.mm(embeddings[j], centroids.t())\n",
    "                distances, assignments = dot_products.max(dim=1)\n",
    "\n",
    "                # finish\n",
    "                if n_iter == nmb_kmeans_iters:\n",
    "                    break\n",
    "\n",
    "                # M step\n",
    "                where_helper = _get_indices_sparse(assignments.cpu().numpy())\n",
    "                counts = (\n",
    "                    torch.zeros(K).to(device=device, non_blocking=True).int()\n",
    "                )\n",
    "                emb_sums = torch.zeros(K, feature_dimensions).to(\n",
    "                    device=device, non_blocking=True\n",
    "                )\n",
    "                for k in range(len(where_helper)):\n",
    "                    if len(where_helper[k][0]) > 0:\n",
    "                        emb_sums[k] = torch.sum(\n",
    "                            embeddings[j][where_helper[k][0]],\n",
    "                            dim=0,\n",
    "                        )\n",
    "                        counts[k] = len(where_helper[k][0])\n",
    "                mask = counts > 0\n",
    "                centroids[mask] = emb_sums[mask] / counts[mask].unsqueeze(1)\n",
    "\n",
    "                # normalize centroids\n",
    "                centroids = nn.functional.normalize(centroids, dim=1, p=2)\n",
    "\n",
    "            # Copy centroids to model for forwarding\n",
    "            getattr(\n",
    "                model.prototypes,\n",
    "                \"prototypes\" + str(i_K),\n",
    "            ).weight.copy_(centroids)\n",
    "\n",
    "            logger.info(\"embeddings[j=%i]: %s\", j, embeddings[j].size())\n",
    "\n",
    "            # Save results to local tensors\n",
    "            assignments_per_prototype[i_K][indexes] = assignments\n",
    "            indexes_per_prototype[i_K][indexes] = indexes\n",
    "            distances_per_prototype[i_K][indexes] = distances\n",
    "            # For the embeddings, make sure to use j for indexing\n",
    "            embeddings_per_prototype[i_K][j][indexes] = embeddings[j]\n",
    "\n",
    "            j_prev = j\n",
    "            # next memory bank to use\n",
    "            j = (j + 1) % len(crops_for_assign)\n",
    "\n",
    "        epoch_comp = epoch + 1\n",
    "\n",
    "        if (\n",
    "            # Plot for the first epoch\n",
    "            epoch_comp == 1\n",
    "            # Below 100 epochs, plot every 25 epochs,\n",
    "            or (epoch_comp <= 100 and epoch_comp % 25 == 0)\n",
    "            # Plot every hundredth epoch\n",
    "            or epoch_comp % 100 == 0\n",
    "            # Plot for the last epoch\n",
    "            or epoch_comp == n_epochs\n",
    "        ):\n",
    "            # Save which random samples were used as the centroids.\n",
    "            assignments_cpu = assignments_per_prototype[-1].cpu()\n",
    "            a6.plotting.embeddings.plot_embeddings_using_tsne(\n",
    "                embeddings=embeddings_per_prototype[-1],\n",
    "                # Use previous j since this represents which crops\n",
    "                # were used for last cluster iteration.\n",
    "                j=j_prev,\n",
    "                assignments=assignments_cpu,\n",
    "                centroids=random_idx,\n",
    "                name=f\"epoch-{epoch}-embeddings\",\n",
    "                output_dir=plots_path,\n",
    "            )\n",
    "            a6.plotting.assignments.plot_abundance(\n",
    "                assignments=assignments_cpu,\n",
    "                name=f\"epoch-{epoch}-assignments-abundance\",\n",
    "                output_dir=plots_path,\n",
    "            )\n",
    "            a6.plotting.transitions.plot_transition_matrix_heatmap(\n",
    "                assignments_cpu,\n",
    "                name=f\"epoch-{epoch}-transition-heatmap\",\n",
    "                output_dir=plots_path,\n",
    "            )\n",
    "            a6.plotting.transitions.plot_transition_matrix_clustermap(\n",
    "                assignments_cpu,\n",
    "                name=f\"epoch-{epoch}-transition-clustermap\",\n",
    "                output_dir=plots_path,\n",
    "            )\n",
    "\n",
    "    return assignments_per_prototype\n",
    "\n",
    "\n",
    "def _get_indices_sparse(data):\n",
    "    cols = np.arange(data.size)\n",
    "    M = csr_matrix(\n",
    "        (cols, (data.ravel(), cols)), shape=(int(data.max()) + 1, data.size)\n",
    "    )\n",
    "    return [np.unravel_index(row.data, data.shape) for row in M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b8cd8-a1cf-4932-9f46-b98d86597d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    indexes: torch.Tensor,\n",
    "    embeddings: torch.Tensor,\n",
    "    nmb_crops: tuple[float, ...],\n",
    "    crops_for_assign: tuple[float, ...],\n",
    "    nmb_prototypes: tuple[int, ...],\n",
    "    feature_dimensions: int,\n",
    "    n_epochs: int,\n",
    "    nmb_kmeans_iters: int,\n",
    "    temperature: float,\n",
    "    device: torch.device,\n",
    "):\n",
    "    batch_time = averaging.AverageMeter()\n",
    "    data_time = averaging.AverageMeter()\n",
    "    losses = averaging.AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    assignments = cluster_memory(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        indexes=indexes,\n",
    "        embeddings=embeddings,\n",
    "        size_dataset=len(dataloader.dataset),\n",
    "        device=device,\n",
    "        crops_for_assign=crops_for_assign,\n",
    "        nmb_prototypes=nmb_prototypes,\n",
    "        feature_dimensions=feature_dimensions,\n",
    "        nmb_kmeans_iters=nmb_kmeans_iters,\n",
    "        n_epochs=n_epochs,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Clustering for epoch %i done\", epoch)\n",
    "\n",
    "    end = time.time()\n",
    "    start_idx = 0\n",
    "    for it, (idx, inputs) in enumerate(dataloader):\n",
    "        logger.info(\"Calculating loss for index %s\", idx)\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # ============ multi-res forward passes ... ============\n",
    "        # Output here returns the output for each head (prototype)\n",
    "        # and hence has size ``len(settings.model.nmb_prototypes)``.\n",
    "        emb, output = model(inputs)\n",
    "        emb = emb.detach()\n",
    "        bs = inputs[0].size(0)\n",
    "\n",
    "        if bs == 0:\n",
    "            raise RuntimeError(\n",
    "                f\"Batch size is zero, loss will be NaN: it={it}, idx={idx}, \"\n",
    "                \"inputs[0]={inputs[0]}\"\n",
    "            )\n",
    "\n",
    "        logger.info(\"Batch size is %i\", bs)\n",
    "\n",
    "        # ============ deepcluster-v2 loss ... ============\n",
    "        loss = torch.tensor(0.0).to(device=device)\n",
    "        for h in range(len(nmb_prototypes)):\n",
    "            scores = output[h] / temperature\n",
    "            targets = (\n",
    "                assignments[h][idx]\n",
    "                .repeat(sum(nmb_crops))\n",
    "                .to(device=device, non_blocking=True)\n",
    "            )\n",
    "            loss_temp = cross_entropy(scores, targets)\n",
    "            loss += loss_temp\n",
    "\n",
    "            if torch.isnan(loss_temp).any() or torch.isnan(loss).any():\n",
    "                logger.warning(\n",
    "                    (\n",
    "                        \"Loss is NaN: it=%i, prototype(h)=%i, \"\n",
    "                        \"nmb_prototypes=%s, inputs=%s, \"\n",
    "                        \"idx=%s, assignments=%s, output=%s, targets=%s, \"\n",
    "                        \"scores=%s, sum_nmb_crops=%s, loss_temp=%s, loss=%s, \"\n",
    "                        \"loss.item()=%s\"\n",
    "                    ),\n",
    "                    it,\n",
    "                    h,\n",
    "                    nmb_prototypes,\n",
    "                    inputs,\n",
    "                    idx,\n",
    "                    assignments[h][idx],\n",
    "                    output[h],\n",
    "                    targets,\n",
    "                    scores,\n",
    "                    sum(nmb_crops),\n",
    "                    loss_temp,\n",
    "                    loss,\n",
    "                    loss.item(),\n",
    "                )\n",
    "        loss /= len(nmb_prototypes)\n",
    "\n",
    "        # ============ backward and optim step ... ============\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ============ update memory banks ... ============\n",
    "        indexes[start_idx : start_idx + bs] = idx\n",
    "        for i, crop_idx in enumerate(crops_for_assign):\n",
    "            embeddings[i][start_idx : start_idx + bs] = emb[  # noqa: E203\n",
    "                crop_idx * bs : (crop_idx + 1) * bs\n",
    "            ]\n",
    "        start_idx += bs\n",
    "\n",
    "        # ============ misc ... ============\n",
    "        losses.update(loss.item(), bs)\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if it % 50 == 0:\n",
    "            logger.info(\n",
    "                \"[EPOCH %i, ITERATION %i] \"\n",
    "                \"batch time: %s (%s) \"\n",
    "                \"data load time: %s (%s) \"\n",
    "                \"loss: %s (%s) \"\n",
    "                \"lr: %s\",\n",
    "                epoch,\n",
    "                it,\n",
    "                batch_time.val,\n",
    "                batch_time.avg,\n",
    "                data_time.val,\n",
    "                data_time.avg,\n",
    "                losses.val,\n",
    "                losses.avg,\n",
    "                optimizer.state_dict()[\"param_groups\"][0][\"lr\"],\n",
    "            )\n",
    "\n",
    "    logger.info(\n",
    "        \"========= Memory Summary at epoch %i =======\\n%s\\n\",\n",
    "        epoch,\n",
    "        torch.cuda.memory_summary(),\n",
    "    )\n",
    "\n",
    "    return (epoch, losses.avg), indexes, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f711d-4344-4704-bdf4-89c979e6bdb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start(\n",
    "    train_dataset: a6.datasets.crop.Base,\n",
    "    epochs: int,\n",
    "    nmb_crops: tuple[float, ...],\n",
    "    crops_for_assign: tuple[float, ...],\n",
    "    model_architecture: a6.models.resnet.Architecture = a6.models.resnet.Architecture.ResNet50,\n",
    "    batch_size: int = 64,\n",
    "    drop_last: bool = False,\n",
    "    hidden_mlp: int = 2048,\n",
    "    feature_dimensions: int = 128,\n",
    "    nmb_kmeans_iters: int = 10,\n",
    "    nmb_prototypes: int = 3,\n",
    "    nmb_clusters: int = 40,\n",
    "    base_lr: float = 4.8,\n",
    "    weight_decay: float = 1e-6,\n",
    "    temperature: float = 0.1,\n",
    "):\n",
    "    nmb_prototypes = [nmb_clusters for _ in range(nmb_prototypes)]\n",
    "    logger.info(\"Prototypes for model: %s\", nmb_prototypes)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        # ``drop_last=True`` gives each device the same amount of samples,\n",
    "        # but removes some from the clustering.\n",
    "        drop_last=drop_last,\n",
    "        worker_init_fn=a6.utils.distributed.set_dataloader_seeds,\n",
    "    )\n",
    "    logger.info(\"Building data done with %s images loaded\", len(train_dataset))\n",
    "\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    logger.info(\"Using device %s\", device)\n",
    "\n",
    "    # build model\n",
    "    model = a6.models.resnet.Models[model_architecture](\n",
    "        normalize=True,\n",
    "        in_channels=train_dataset.n_channels,\n",
    "        hidden_mlp=hidden_mlp,\n",
    "        output_dim=feature_dimensions,\n",
    "        nmb_prototypes=nmb_prototypes,\n",
    "        device=device,\n",
    "    )\n",
    "    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "\n",
    "    # Copy model to GPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    logger.info(model)\n",
    "    logger.info(\"Building model done\")\n",
    "\n",
    "    # build optimizer\n",
    "    # Should be done after moving the model to GPU\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=base_lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Building optimizer done\")\n",
    "    training_stats = logs.Stats(\"stats.csv\", columns=(\"epoch\", \"loss\"))\n",
    "\n",
    "    indexes, embeddings = init_memory(\n",
    "        dataloader=train_loader,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        feature_dimensions=feature_dimensions,\n",
    "        crops_for_assign=crops_for_assign,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # train the network for one epoch\n",
    "        logger.info(f\"============ Starting epoch %i ============\", epoch)\n",
    "\n",
    "        # set sampler\n",
    "        # train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        # train the network\n",
    "        scores, indexes, embeddings = train(\n",
    "            dataloader=train_loader,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            n_epochs=epochs,\n",
    "            indexes=indexes,\n",
    "            embeddings=embeddings,\n",
    "            nmb_crops=nmb_crops,\n",
    "            crops_for_assign=crops_for_assign,\n",
    "            nmb_prototypes=nmb_prototypes,\n",
    "            feature_dimensions=feature_dimensions,\n",
    "            nmb_kmeans_iters=nmb_kmeans_iters,\n",
    "            device=device,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        training_stats.update(scores)\n",
    "\n",
    "\n",
    "start(\n",
    "    train_dataset=train_dataset,\n",
    "    epochs=10,\n",
    "    nmb_crops=nmb_crops,\n",
    "    crops_for_assign=crops_for_assign,\n",
    "    batch_size=64,\n",
    "    nmb_prototypes=3,\n",
    "    nmb_clusters=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717224d3-0ae6-4ee0-a341-0ca0b7b7189c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maelstrom-bootcamp-2023",
   "language": "python",
   "name": "maelstrom-bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
