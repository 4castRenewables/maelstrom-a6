{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec5ac82-15e4-4d80-98bb-afc673f9c19b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import scipy.constants as constants\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xarray as xr\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import a6\n",
    "import a6.dcv2._logs as logs\n",
    "import a6.dcv2._averaging as averaging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "logger = logging.getLogger(\"notebook\")\n",
    "logger.info(\"Logger initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63874afa-4a9d-4985-a73e-783ffe6dc4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@a6.utils.make_functional\n",
    "def calculate_geopotential_height(\n",
    "    data: xr.Dataset,\n",
    "    scaling: float = 10.0,\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Calculate the geopotential height from the geopotential.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    data : xr.Dataset\n",
    "        Data containing the geopotential.\n",
    "    scaling : float, default=10.0\n",
    "        Parameter used for scaling the data.\n",
    "        E.g. the ERA5 geopotential is given in decameters.\n",
    "\n",
    "    \"\"\"\n",
    "    data[\"z_h\"] = data[\"z\"] / constants.g / scaling\n",
    "    return data\n",
    "\n",
    "\n",
    "@a6.utils.make_functional\n",
    "def drop_variables_from_dataset(\n",
    "    data: xr.Dataset,\n",
    "    names: str | list[str],\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Drop variables from dataset.\"\"\"\n",
    "    return data.drop_vars(names)\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    paths: pathlib.Path | list[pathlib.Path],\n",
    "    is_netcdf: bool,\n",
    "    nmb_crops: tuple[int, ...],\n",
    "    size_crops: tuple[float, ...],\n",
    "    min_scale_crops: tuple[float, ...],\n",
    "    max_scale_crops: tuple[float, ...],\n",
    "    drop_variables: list[str] | None = None,\n",
    ") -> a6.datasets.crop.Base:\n",
    "    if is_netcdf:\n",
    "        logger.info(\"Assuming dataset from netCDF files\")\n",
    "        drop_variables = drop_variables or []\n",
    "\n",
    "        preprocessing = (\n",
    "            a6.features.methods.weighting.weight_by_latitudes(\n",
    "                latitudes=\"latitude\",\n",
    "                use_sqrt=True,\n",
    "            )\n",
    "            >> calculate_geopotential_height()\n",
    "            >> drop_variables_from_dataset(names=[\"z\"])\n",
    "        )\n",
    "        logger.info(\"Reading data from netCDF files %s\", paths)\n",
    "        ds = xr.open_mfdataset(\n",
    "            paths,\n",
    "            engine=\"netcdf4\",\n",
    "            concat_dim=\"time\",\n",
    "            combine=\"nested\",\n",
    "            coords=\"minimal\",\n",
    "            data_vars=\"minimal\",\n",
    "            preprocess=preprocessing,\n",
    "            drop_variables=drop_variables,\n",
    "            compat=\"override\",\n",
    "            parallel=False,\n",
    "        )\n",
    "        return a6.datasets.crop.MultiCropXarrayDataset(\n",
    "            data_path=path,\n",
    "            dataset=ds,\n",
    "            nmb_crops=nmb_crops,\n",
    "            size_crops=size_crops,\n",
    "            min_scale_crops=min_scale_crops,\n",
    "            max_scale_crops=max_scale_crops,\n",
    "            return_index=True,\n",
    "        )\n",
    "    logger.info(\"Assuming image folder dataset\")\n",
    "    return a6.datasets.crop.MultiCropDataset(\n",
    "        data_path=path,\n",
    "        nmb_crops=nmb_crops,\n",
    "        size_crops=size_crops,\n",
    "        min_scale_crops=min_scale_crops,\n",
    "        max_scale_crops=max_scale_crops,\n",
    "        return_index=True,\n",
    "    )\n",
    "\n",
    "\n",
    "path = pathlib.Path(\n",
    "    \"/p/project/training2330/a6/data/ecmwf_era5/nc/era5_pl_2012_2023_12.nc\"\n",
    ")\n",
    "nmb_crops = (2,)\n",
    "crops_for_assign = (0, 1)\n",
    "size_crops = (0.75,)\n",
    "min_scale_crops = (0.15,)\n",
    "max_scale_crops = (1.0,)\n",
    "\n",
    "train_dataset = create_dataset(\n",
    "    paths=[path],\n",
    "    is_netcdf=True,\n",
    "    nmb_crops=nmb_crops,\n",
    "    size_crops=size_crops,\n",
    "    min_scale_crops=min_scale_crops,\n",
    "    max_scale_crops=max_scale_crops,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c47d33-7d0f-4290-ad46-5089141421f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_memory(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: nn.Module,\n",
    "    device: torch.device,\n",
    "    feature_dimensions: int,\n",
    "    crops_for_assign: tuple[int, float],\n",
    "    drop_last: bool = False,\n",
    "):\n",
    "    size_dataset = len(dataloader.dataset)\n",
    "    logger.info(\"Dataset size is %i samples\", size_dataset)\n",
    "\n",
    "    if drop_last:\n",
    "        size_dataset -= size_dataset % settings.model.batch_size\n",
    "        logger.warning(\n",
    "            \"Adjusted size of memory per process due to drop_last=True to %i\",\n",
    "            size_dataset,\n",
    "        )\n",
    "\n",
    "    logger.info(\"Processing %i samples\", size_dataset)\n",
    "\n",
    "    indexes = torch.zeros(size_dataset).long().to(device=device)\n",
    "    embeddings = torch.zeros(\n",
    "        len(crops_for_assign),\n",
    "        size_dataset,\n",
    "        feature_dimensions,\n",
    "    ).to(device=device)\n",
    "\n",
    "    start_idx = 0\n",
    "    with torch.no_grad():\n",
    "        logger.info(\"Start initializing the memory banks\")\n",
    "        for index, inputs in dataloader:\n",
    "            logger.info(\n",
    "                \"Processing %i samples from data indexes %s\",\n",
    "                index.size(0),\n",
    "                index,\n",
    "            )\n",
    "            n_indexes = inputs[0].size(0)\n",
    "            index = index.to(device=device, non_blocking=True)\n",
    "\n",
    "            # get embeddings\n",
    "            outputs = []\n",
    "            for crop_idx in crops_for_assign:\n",
    "                inp = inputs[crop_idx].to(device=device, non_blocking=True)\n",
    "                outputs.append(model(inp)[0])\n",
    "\n",
    "            # fill the memory bank\n",
    "            indexes[start_idx : start_idx + n_indexes] = index\n",
    "            for mb_idx, embedding in enumerate(outputs):\n",
    "                embeddings[mb_idx][\n",
    "                    start_idx : start_idx + n_indexes\n",
    "                ] = embedding\n",
    "            start_idx += n_indexes\n",
    "    logger.info(\n",
    "        \"Initialization of the memory banks done with %s local memory indexes\",\n",
    "        indexes.size(),\n",
    "    )\n",
    "    return indexes, embeddings\n",
    "\n",
    "\n",
    "def cluster_memory(\n",
    "    epoch: int,\n",
    "    model,\n",
    "    indexes: torch.Tensor,\n",
    "    embeddings: torch.Tensor,\n",
    "    size_dataset: int,\n",
    "    device: torch.device,\n",
    "    crops_for_assign: tuple[float, ...],\n",
    "    nmb_prototypes: tuple[int, ...],\n",
    "    feature_dimensions: int,\n",
    "    n_epochs: int,\n",
    "    nmb_kmeans_iters: int,\n",
    "    plots_path: pathlib.Path = pathlib.Path(\".\"),\n",
    "):\n",
    "    logger.info(\"Clustering %i samples\", size_dataset)\n",
    "\n",
    "    # j defines which crops are used for the K-means run.\n",
    "    # E.g. if the number of crops (``self.nmb_mbs``) is 2, and\n",
    "    # ``self.num_clusters = [30, 30, 30, 30]``, the crops will\n",
    "    # be used as following:\n",
    "    #\n",
    "    # 1. K=30, j=0\n",
    "    # 2. K=30, j=1\n",
    "    # 3. K=30, j=0\n",
    "    # 4. K=30, j=1\n",
    "    j = 0\n",
    "\n",
    "    n_heads = len(nmb_prototypes)\n",
    "\n",
    "    assignments_per_prototype = (torch.zeros(n_heads, size_dataset).long()).to(\n",
    "        device\n",
    "    )\n",
    "    indexes_per_prototype = torch.zeros(n_heads, size_dataset).long().to(device)\n",
    "\n",
    "    embeddings_per_prototype = torch.zeros(\n",
    "        n_heads,\n",
    "        *tuple(embeddings.size()),\n",
    "    ).to(device)\n",
    "    distances_per_prototype = torch.zeros(n_heads, size_dataset).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i_K, K in enumerate(nmb_prototypes):\n",
    "            # run k-means\n",
    "\n",
    "            # init with random samples as centroids from the dataset\n",
    "            centroids = torch.empty(K, feature_dimensions).to(\n",
    "                device=device, non_blocking=True\n",
    "            )\n",
    "\n",
    "            batch_size = len(embeddings[j])\n",
    "            random_idx = torch.randperm(batch_size)[:K]\n",
    "            assert len(random_idx) >= K, (\n",
    "                f\"Please reduce the number of centroids K={K}: \"\n",
    "                f\"K must be smaller than batch size {batch_size}\"\n",
    "            )\n",
    "            centroids = embeddings[j][random_idx]\n",
    "\n",
    "            for n_iter in range(nmb_kmeans_iters + 1):\n",
    "                # E step\n",
    "                dot_products = torch.mm(embeddings[j], centroids.t())\n",
    "                distances, assignments = dot_products.max(dim=1)\n",
    "\n",
    "                # finish\n",
    "                if n_iter == nmb_kmeans_iters:\n",
    "                    break\n",
    "\n",
    "                # M step\n",
    "                where_helper = _get_indices_sparse(assignments.cpu().numpy())\n",
    "                counts = (\n",
    "                    torch.zeros(K).to(device=device, non_blocking=True).int()\n",
    "                )\n",
    "                emb_sums = torch.zeros(K, feature_dimensions).to(\n",
    "                    device=device, non_blocking=True\n",
    "                )\n",
    "                for k in range(len(where_helper)):\n",
    "                    if len(where_helper[k][0]) > 0:\n",
    "                        emb_sums[k] = torch.sum(\n",
    "                            embeddings[j][where_helper[k][0]],\n",
    "                            dim=0,\n",
    "                        )\n",
    "                        counts[k] = len(where_helper[k][0])\n",
    "                mask = counts > 0\n",
    "                centroids[mask] = emb_sums[mask] / counts[mask].unsqueeze(1)\n",
    "\n",
    "                # normalize centroids\n",
    "                centroids = nn.functional.normalize(centroids, dim=1, p=2)\n",
    "\n",
    "            # Copy centroids to model for forwarding\n",
    "            getattr(\n",
    "                model.prototypes,\n",
    "                \"prototypes\" + str(i_K),\n",
    "            ).weight.copy_(centroids)\n",
    "\n",
    "            logger.info(\"embeddings[j=%i]: %s\", j, embeddings[j].size())\n",
    "\n",
    "            # Save results to local tensors\n",
    "            assignments_per_prototype[i_K][indexes] = assignments\n",
    "            indexes_per_prototype[i_K][indexes] = indexes\n",
    "            distances_per_prototype[i_K][indexes] = distances\n",
    "            # For the embeddings, make sure to use j for indexing\n",
    "            embeddings_per_prototype[i_K][j][indexes] = embeddings[j]\n",
    "\n",
    "            j_prev = j\n",
    "            # next memory bank to use\n",
    "            j = (j + 1) % len(crops_for_assign)\n",
    "\n",
    "        epoch_comp = epoch + 1\n",
    "\n",
    "        if (\n",
    "            # Plot for the first epoch\n",
    "            epoch_comp == 1\n",
    "            # Below 100 epochs, plot every 25 epochs,\n",
    "            or (epoch_comp <= 100 and epoch_comp % 25 == 0)\n",
    "            # Plot every hundredth epoch\n",
    "            or epoch_comp % 100 == 0\n",
    "            # Plot for the last epoch\n",
    "            or epoch_comp == n_epochs\n",
    "        ):\n",
    "            # Save which random samples were used as the centroids.\n",
    "            assignments_cpu = assignments_per_prototype[-1].cpu()\n",
    "            a6.plotting.embeddings.plot_embeddings_using_tsne(\n",
    "                embeddings=embeddings_per_prototype[-1],\n",
    "                # Use previous j since this represents which crops\n",
    "                # were used for last cluster iteration.\n",
    "                j=j_prev,\n",
    "                assignments=assignments_cpu,\n",
    "                centroids=random_idx,\n",
    "                name=f\"epoch-{epoch}-embeddings\",\n",
    "                output_dir=plots_path,\n",
    "            )\n",
    "            a6.plotting.assignments.plot_abundance(\n",
    "                assignments=assignments_cpu,\n",
    "                name=f\"epoch-{epoch}-assignments-abundance\",\n",
    "                output_dir=plots_path,\n",
    "            )\n",
    "            a6.plotting.transitions.plot_transition_matrix_heatmap(\n",
    "                assignments_cpu,\n",
    "                name=f\"epoch-{epoch}-transition-heatmap\",\n",
    "                output_dir=plots_path,\n",
    "            )\n",
    "            a6.plotting.transitions.plot_transition_matrix_clustermap(\n",
    "                assignments_cpu,\n",
    "                name=f\"epoch-{epoch}-transition-clustermap\",\n",
    "                output_dir=plots_path,\n",
    "            )\n",
    "\n",
    "    return assignments_per_prototype\n",
    "\n",
    "\n",
    "def _get_indices_sparse(data):\n",
    "    cols = np.arange(data.size)\n",
    "    M = csr_matrix(\n",
    "        (cols, (data.ravel(), cols)), shape=(int(data.max()) + 1, data.size)\n",
    "    )\n",
    "    return [np.unravel_index(row.data, data.shape) for row in M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b8cd8-a1cf-4932-9f46-b98d86597d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    indexes: torch.Tensor,\n",
    "    embeddings: torch.Tensor,\n",
    "    nmb_crops: tuple[float, ...],\n",
    "    crops_for_assign: tuple[float, ...],\n",
    "    nmb_prototypes: tuple[int, ...],\n",
    "    feature_dimensions: int,\n",
    "    n_epochs: int,\n",
    "    nmb_kmeans_iters: int,\n",
    "    temperature: float,\n",
    "    device: torch.device,\n",
    "):\n",
    "    batch_time = averaging.AverageMeter()\n",
    "    data_time = averaging.AverageMeter()\n",
    "    losses = averaging.AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    assignments = cluster_memory(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        indexes=indexes,\n",
    "        embeddings=embeddings,\n",
    "        size_dataset=len(dataloader.dataset),\n",
    "        device=device,\n",
    "        crops_for_assign=crops_for_assign,\n",
    "        nmb_prototypes=nmb_prototypes,\n",
    "        feature_dimensions=feature_dimensions,\n",
    "        nmb_kmeans_iters=nmb_kmeans_iters,\n",
    "        n_epochs=n_epochs,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Clustering for epoch %i done\", epoch)\n",
    "\n",
    "    end = time.time()\n",
    "    start_idx = 0\n",
    "    for it, (idx, inputs) in enumerate(dataloader):\n",
    "        logger.info(\"Calculating loss for index %s\", idx)\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # ============ multi-res forward passes ... ============\n",
    "        # Output here returns the output for each head (prototype)\n",
    "        # and hence has size ``len(settings.model.nmb_prototypes)``.\n",
    "        emb, output = model(inputs)\n",
    "        emb = emb.detach()\n",
    "        bs = inputs[0].size(0)\n",
    "\n",
    "        if bs == 0:\n",
    "            raise RuntimeError(\n",
    "                f\"Batch size is zero, loss will be NaN: it={it}, idx={idx}, \"\n",
    "                \"inputs[0]={inputs[0]}\"\n",
    "            )\n",
    "\n",
    "        logger.info(\"Batch size is %i\", bs)\n",
    "\n",
    "        # ============ deepcluster-v2 loss ... ============\n",
    "        loss = torch.tensor(0.0)\n",
    "        for h in range(len(nmb_prototypes)):\n",
    "            scores = output[h] / temperature\n",
    "            targets = (\n",
    "                assignments[h][idx]\n",
    "                .repeat(sum(nmb_crops))\n",
    "                .to(device=device, non_blocking=True)\n",
    "            )\n",
    "            loss_temp = cross_entropy(scores, targets)\n",
    "            loss += loss_temp\n",
    "\n",
    "            if torch.isnan(loss_temp).any() or torch.isnan(loss).any():\n",
    "                logger.warning(\n",
    "                    (\n",
    "                        \"Loss is NaN: it=%i, prototype(h)=%i, \"\n",
    "                        \"nmb_prototypes=%s, \"\n",
    "                        \"idx=%s, assignments=%s, output=%s, targets=%s, \"\n",
    "                        \"scores=%s, sum_nmb_crops=%s, loss_temp=%s, loss=%s, \"\n",
    "                        \"loss.item()=%s\"\n",
    "                    ),\n",
    "                    it,\n",
    "                    h,\n",
    "                    nmb_prototypes,\n",
    "                    idx,\n",
    "                    assignments[h][idx],\n",
    "                    output[h],\n",
    "                    targets,\n",
    "                    scores,\n",
    "                    sum(nmb_crops),\n",
    "                    loss_temp,\n",
    "                    loss,\n",
    "                    loss.item(),\n",
    "                )\n",
    "        loss /= len(nmb_prototypes)\n",
    "\n",
    "        # ============ backward and optim step ... ============\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ============ update memory banks ... ============\n",
    "        indexes[start_idx : start_idx + bs] = idx\n",
    "        for i, crop_idx in enumerate(crops_for_assign):\n",
    "            embeddings[i][start_idx : start_idx + bs] = emb[  # noqa: E203\n",
    "                crop_idx * bs : (crop_idx + 1) * bs\n",
    "            ]\n",
    "        start_idx += bs\n",
    "\n",
    "        # ============ misc ... ============\n",
    "        losses.update(loss.item(), bs)\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if it % 50 == 0:\n",
    "            logger.info(\n",
    "                \"[EPOCH %i, ITERATION %i] \"\n",
    "                \"batch time: %s (%s) \"\n",
    "                \"data load time: %s (%s) \"\n",
    "                \"loss: %s (%s) \"\n",
    "                \"lr: %s\",\n",
    "                epoch,\n",
    "                it,\n",
    "                batch_time.val,\n",
    "                batch_time.avg,\n",
    "                data_time.val,\n",
    "                data_time.avg,\n",
    "                losses.val,\n",
    "                losses.avg,\n",
    "                optimizer.state_dict()[\"param_groups\"][0][\"lr\"],\n",
    "            )\n",
    "\n",
    "    logger.info(\n",
    "        \"========= Memory Summary at epoch %i =======\\n%s\\n\",\n",
    "        epoch,\n",
    "        torch.cuda.memory_summary(),\n",
    "    )\n",
    "\n",
    "    return (epoch, losses.avg), indexes, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f711d-4344-4704-bdf4-89c979e6bdb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start(\n",
    "    train_dataset: a6.datasets.crop.Base,\n",
    "    epochs: int,\n",
    "    nmb_crops: tuple[float, ...],\n",
    "    crops_for_assign: tuple[float, ...],\n",
    "    model_architecture: a6.models.resnet.Architecture = a6.models.resnet.Architecture.ResNet50,\n",
    "    batch_size: int = 64,\n",
    "    drop_last: bool = False,\n",
    "    hidden_mlp: int = 2048,\n",
    "    feature_dimensions: int = 128,\n",
    "    nmb_kmeans_iters: int = 10,\n",
    "    nmb_prototypes: int = 3,\n",
    "    nmb_clusters: int = 40,\n",
    "    base_lr: float = 4.8,\n",
    "    weight_decay: float = 1e-6,\n",
    "    temperature: float = 0.1,\n",
    "):\n",
    "    nmb_prototypes = [nmb_clusters for _ in range(nmb_prototypes)]\n",
    "    logger.info(\"Prototypes for model: %s\", nmb_prototypes)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        # ``drop_last=True`` gives each device the same amount of samples,\n",
    "        # but removes some from the clustering.\n",
    "        drop_last=drop_last,\n",
    "        worker_init_fn=a6.utils.distributed.set_dataloader_seeds,\n",
    "    )\n",
    "    logger.info(\"Building data done with %s images loaded\", len(train_dataset))\n",
    "\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    # build model\n",
    "    model = a6.models.resnet.Models[model_architecture](\n",
    "        normalize=True,\n",
    "        in_channels=train_dataset.n_channels,\n",
    "        hidden_mlp=hidden_mlp,\n",
    "        output_dim=feature_dimensions,\n",
    "        nmb_prototypes=nmb_prototypes,\n",
    "        device=device,\n",
    "    )\n",
    "    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "\n",
    "    # Copy model to GPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    logger.info(model)\n",
    "    logger.info(\"Building model done\")\n",
    "\n",
    "    # build optimizer\n",
    "    # Should be done after moving the model to GPU\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=base_lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Building optimizer done\")\n",
    "    training_stats = logs.Stats(\"stats.csv\", columns=(\"epoch\", \"loss\"))\n",
    "\n",
    "    indexes, embeddings = init_memory(\n",
    "        dataloader=train_loader,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        feature_dimensions=feature_dimensions,\n",
    "        crops_for_assign=crops_for_assign,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # train the network for one epoch\n",
    "        logger.info(f\"============ Starting epoch %i ============\", epoch)\n",
    "\n",
    "        # set sampler\n",
    "        # train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        # train the network\n",
    "        scores, indexes, embeddings = train(\n",
    "            dataloader=train_loader,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            n_epochs=epochs,\n",
    "            indexes=indexes,\n",
    "            embeddings=embeddings,\n",
    "            nmb_crops=nmb_crops,\n",
    "            crops_for_assign=crops_for_assign,\n",
    "            nmb_prototypes=nmb_prototypes,\n",
    "            feature_dimensions=feature_dimensions,\n",
    "            nmb_kmeans_iters=nmb_kmeans_iters,\n",
    "            device=device,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        training_stats.update(scores)\n",
    "\n",
    "\n",
    "start(\n",
    "    train_dataset=train_dataset,\n",
    "    epochs=10,\n",
    "    nmb_crops=nmb_crops,\n",
    "    crops_for_assign=crops_for_assign,\n",
    "    batch_size=64,\n",
    "    nmb_prototypes=3,\n",
    "    nmb_clusters=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717224d3-0ae6-4ee0-a341-0ca0b7b7189c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maelstrom-bootcamp-2023",
   "language": "python",
   "name": "maelstrom-bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
